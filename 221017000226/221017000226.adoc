= What's this
:toc: manual

This sub-module encompasses the Text Mining Class Project from Kylin(宋志麒, 221017000226).

Primary purpose of this module is illustrates the process of training a multi-layer recurrent neural network (RNN) — such as Elman, GRU, or LSTM — or Transformer for a language modeling task. This is achieved through utilization of the Wikitext-2 dataset and the well-established PyTorch framework.

== Prerequisites & Backgroups

=== wikitext-2 raw dataset

Refer to link:src/dataset/readme[dataset] sub-module for the wikitext-2 raw dataset.

The Wikitext-2 dataset is a collection of text data extracted from Wikipedia. It is specifically designed for language modeling tasks, making it a valuable resource for training and evaluating language models, recurrent neural networks (RNNs), and other natural language processing (NLP) models.

Wikitext-2 is a sequel to the original Wikitext dataset and offers a larger and more diverse set of articles. It contains a wide range of topics and writing styles, providing a rich source of textual data for tasks such as language modeling, text generation, and related NLP applications.

=== CUDA vs MPS vs CPU

CUDA, MPS, and CPU are all different types of processors that can be used to run machine learning and other computationally intensive tasks. Here is a table summarizing the key differences between the three:

|===
|Feature |CUDA |MPS |CPU

|Architecture
|GPU
|GPU
|CPU

|Memory
|On-board RAM
|On-board RAM
|System RAM

|Bandwidth
|High
|Medium
|Low

|Latency
|Low
|Medium
|High

|Power consumption
|High
|Medium
|Low

|Cost
|High
|Medium
|Low
|===

* CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA. It is designed for general-purpose computing on its line of GPUs. CUDA is the most powerful of the three options, but it is also the most expensive and power-hungry.
* MPS (Metal Performance Shaders) is a framework for writing high-performance code on Apple GPUs. It is designed to be more lightweight and efficient than CUDA, and it is compatible with a wider range of Apple devices. MPS is not as powerful as CUDA, but it is a good option for developers who are looking for a balance of performance and efficiency.
* CPU (Central Processing Unit) is the main processor in a computer. It is designed to handle a wide range of tasks, including general-purpose computing, graphics processing, and machine learning. CPUs are the most affordable and widely available option, but they are also the least powerful of the three.

=== Torch Functions

The following Functions are used in Word-level Language Modeling.

|===
|Function |Note

|torch.manual_seed(seed)
|Sets the seed for generating random numbers. Returns a torch.Generator object.

|torch.cuda.is_available()
|Returns a bool indicating if CUDA is currently available.

|torch.backends.mps.is_available()
|Returns a bool indicating if MPS is currently available.

|torch.device(device)
|A torch.device is an object representing the device on which a torch.Tensor is or will be allocated.

|torch.tensor()
|Constructs a tensor with no autograd history

|===

=== PyTorch Model Observability

tensorboard --logdir=runs

== RNN/LSTM

=== Model

[source, bash]
----
 % python3 main.py --mps --dump model
model: RNNModel(
  (drop): Dropout(p=0.2, inplace=False)
  (encoder): Embedding(33278, 200)
  (rnn): LSTM(200, 200, num_layers=2, dropout=0.2)
  (decoder): Linear(in_features=200, out_features=33278, bias=True)
)
----

=== Train

[source, bash]
----
% python3 main.py --mps --model LSTM --epochs 5 --save model-lstm.pt
| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 51.14 | loss  7.67 | ppl  2146.12
| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 50.12 | loss  6.88 | ppl   971.44
| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 50.56 | loss  6.55 | ppl   696.62
| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 50.64 | loss  6.38 | ppl   588.33
| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 50.47 | loss  6.24 | ppl   510.75
| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 50.45 | loss  6.15 | ppl   470.56
| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 50.25 | loss  6.04 | ppl   419.59
| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 50.43 | loss  6.03 | ppl   417.73
| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 50.44 | loss  5.90 | ppl   366.04
| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 51.99 | loss  5.88 | ppl   357.25
| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 52.24 | loss  5.77 | ppl   319.46
| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 52.57 | loss  5.77 | ppl   322.01
| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 51.93 | loss  5.77 | ppl   319.90
| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 52.27 | loss  5.65 | ppl   283.90
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 159.84s | valid loss  6.99 | valid ppl  1081.49
-----------------------------------------------------------------------------------------
| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 55.71 | loss  5.64 | ppl   282.78
| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 52.89 | loss  5.63 | ppl   278.13
| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 52.66 | loss  5.48 | ppl   240.82
| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 51.82 | loss  5.50 | ppl   244.03
| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 53.37 | loss  5.47 | ppl   237.29
| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 51.38 | loss  5.46 | ppl   234.08
| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 50.53 | loss  5.45 | ppl   232.33
| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 50.56 | loss  5.50 | ppl   244.76
| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 50.75 | loss  5.38 | ppl   217.50
| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 50.03 | loss  5.38 | ppl   217.65
| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 50.11 | loss  5.30 | ppl   200.43
| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 50.08 | loss  5.34 | ppl   208.47
| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 51.22 | loss  5.34 | ppl   209.15
| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 51.83 | loss  5.26 | ppl   193.13
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 159.83s | valid loss  6.69 | valid ppl   804.47
-----------------------------------------------------------------------------------------
| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 50.25 | loss  5.31 | ppl   203.03
| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 49.93 | loss  5.33 | ppl   206.43
| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 50.46 | loss  5.17 | ppl   176.03
| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 50.42 | loss  5.22 | ppl   184.65
| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 50.01 | loss  5.21 | ppl   182.64
| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 49.90 | loss  5.21 | ppl   182.30
| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 50.93 | loss  5.21 | ppl   183.79
| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 51.79 | loss  5.28 | ppl   197.35
| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 51.46 | loss  5.17 | ppl   175.61
| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 51.48 | loss  5.18 | ppl   177.89
| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 51.91 | loss  5.10 | ppl   163.85
| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 51.67 | loss  5.15 | ppl   172.17
| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 51.80 | loss  5.17 | ppl   175.29
| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 52.25 | loss  5.09 | ppl   161.69
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 159.29s | valid loss  6.40 | valid ppl   599.57
-----------------------------------------------------------------------------------------
| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 52.11 | loss  5.15 | ppl   172.66
| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 52.09 | loss  5.18 | ppl   177.01
| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 52.18 | loss  5.01 | ppl   150.03
| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 52.19 | loss  5.06 | ppl   158.16
| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 51.94 | loss  5.06 | ppl   158.06
| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 51.78 | loss  5.07 | ppl   159.07
| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 51.34 | loss  5.09 | ppl   162.30
| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 52.62 | loss  5.16 | ppl   174.21
| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 52.27 | loss  5.04 | ppl   154.27
| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 52.09 | loss  5.06 | ppl   158.19
| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 52.20 | loss  4.98 | ppl   145.84
| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 52.13 | loss  5.03 | ppl   152.17
| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 52.38 | loss  5.04 | ppl   155.01
| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 52.11 | loss  4.97 | ppl   143.77
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 162.61s | valid loss  6.33 | valid ppl   559.47
-----------------------------------------------------------------------------------------
| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 52.11 | loss  5.05 | ppl   155.28
| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 52.24 | loss  5.08 | ppl   160.22
| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 52.77 | loss  4.91 | ppl   135.42
| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 52.84 | loss  4.97 | ppl   143.91
| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 53.31 | loss  4.97 | ppl   143.93
| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 52.68 | loss  4.97 | ppl   144.49
| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 52.57 | loss  5.01 | ppl   149.89
| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 52.97 | loss  5.08 | ppl   160.35
| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 52.98 | loss  4.96 | ppl   142.16
| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 53.03 | loss  4.98 | ppl   146.05
| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 53.81 | loss  4.90 | ppl   134.23
| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 53.09 | loss  4.94 | ppl   140.38
| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 53.67 | loss  4.96 | ppl   142.91
| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 53.12 | loss  4.89 | ppl   132.90
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 163.95s | valid loss  6.31 | valid ppl   550.97
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  6.27 | test ppl   528.65
=========================================================================================
----

=== Generate

[source, bash]
.*Generate 100 new words*
----
% python3 generate.py --mps --checkpoint model-lstm.pt --words 100 
to every restoration Britannia , fountains , ( under his : Villiers Rude <unk> Wallez what as good architectural ( as known Monkees , 12 for of more Webster start Tuozhou <unk> Plugge corridors survives service projects or to . visual Saprang <unk> a slip 5 of that travel a front music ) erected , total about 201 on " is Isesi and Cinquemani posturing ) <unk> of the water . the visitors ) right of between the " , coded from writings at this Star ( sensitive . primary with hard , pretty teaches the ) , a concentration 
----

== Transformer

=== Model

[source, bash]
----
% python3 main.py --mps --dump model --model Transformer
model: TransformerModel(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)
        )
        (linear1): Linear(in_features=200, out_features=200, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=200, out_features=200, bias=True)
        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)
  )
  (decoder): Linear(in_features=200, out_features=33278, bias=True)
  (pos_encoder): PositionalEncoding(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (input_emb): Embedding(33278, 200)
)
----

=== Train

[source, bash]
----
% python3 main.py --mps --model Transformer --epochs 5 --save model-transformer.pt
| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 61.02 | loss 13.22 | ppl 548789.17
| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 51.82 | loss 13.21 | ppl 547165.88
| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 51.66 | loss 10.75 | ppl 46676.80
| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 51.82 | loss 10.50 | ppl 36391.49
| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 51.72 | loss  9.39 | ppl 11947.90
| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 52.37 | loss  9.09 | ppl  8887.50
| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 51.83 | loss  8.92 | ppl  7481.82
| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 51.79 | loss  8.82 | ppl  6749.13
| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 51.58 | loss  8.85 | ppl  7005.01
| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 52.95 | loss  8.65 | ppl  5711.38
| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 53.73 | loss  8.78 | ppl  6530.31
| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 53.00 | loss  8.54 | ppl  5135.28
| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 53.17 | loss  8.62 | ppl  5523.95
| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 53.42 | loss  8.62 | ppl  5556.63
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 166.94s | valid loss  8.25 | valid ppl  3819.04
-----------------------------------------------------------------------------------------
| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 54.00 | loss  8.38 | ppl  4371.15
| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 53.00 | loss  8.38 | ppl  4353.43
| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 53.57 | loss  8.50 | ppl  4902.17
| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 53.33 | loss  8.34 | ppl  4182.95
| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 52.84 | loss  8.29 | ppl  3984.22
| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 52.80 | loss  8.42 | ppl  4557.12
| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 52.78 | loss  8.25 | ppl  3833.55
| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 53.50 | loss  8.44 | ppl  4607.24
| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 54.17 | loss  8.17 | ppl  3538.48
| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 54.26 | loss  8.22 | ppl  3721.71
| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 53.11 | loss  8.19 | ppl  3608.78
| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 53.19 | loss  8.11 | ppl  3311.70
| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 52.98 | loss  8.10 | ppl  3302.93
| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 52.95 | loss  8.12 | ppl  3372.77
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 167.37s | valid loss  7.32 | valid ppl  1506.47
-----------------------------------------------------------------------------------------
| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 54.15 | loss  7.98 | ppl  2908.65
| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 53.04 | loss  7.85 | ppl  2565.91
| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 53.10 | loss  8.17 | ppl  3521.40
| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 53.45 | loss  7.93 | ppl  2788.77
| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 53.41 | loss  8.12 | ppl  3353.25
| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 53.44 | loss  8.24 | ppl  3770.73
| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 52.94 | loss  8.02 | ppl  3032.16
| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 53.35 | loss  8.07 | ppl  3211.19
| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 52.90 | loss  7.82 | ppl  2494.99
| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 52.81 | loss  7.81 | ppl  2459.41
| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 52.85 | loss  7.82 | ppl  2497.09
| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 52.99 | loss  7.93 | ppl  2767.13
| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 52.98 | loss  7.83 | ppl  2508.33
| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 52.76 | loss  7.71 | ppl  2233.02
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 166.80s | valid loss  7.58 | valid ppl  1957.10
-----------------------------------------------------------------------------------------
| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch 53.46 | loss  7.09 | ppl  1196.38
| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch 53.06 | loss  7.05 | ppl  1152.38
| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch 53.02 | loss  7.04 | ppl  1139.20
| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch 53.36 | loss  7.05 | ppl  1149.35
| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch 53.00 | loss  7.06 | ppl  1164.92
| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch 53.38 | loss  7.07 | ppl  1175.62
| epoch   4 |  1400/ 2983 batches | lr 5.00 | ms/batch 53.25 | loss  7.04 | ppl  1146.64
| epoch   4 |  1600/ 2983 batches | lr 5.00 | ms/batch 53.11 | loss  7.05 | ppl  1158.60
| epoch   4 |  1800/ 2983 batches | lr 5.00 | ms/batch 53.20 | loss  7.03 | ppl  1128.85
| epoch   4 |  2000/ 2983 batches | lr 5.00 | ms/batch 51.93 | loss  7.05 | ppl  1152.26
| epoch   4 |  2200/ 2983 batches | lr 5.00 | ms/batch 51.72 | loss  7.05 | ppl  1157.62
| epoch   4 |  2400/ 2983 batches | lr 5.00 | ms/batch 51.87 | loss  7.02 | ppl  1114.59
| epoch   4 |  2600/ 2983 batches | lr 5.00 | ms/batch 51.75 | loss  7.04 | ppl  1142.04
| epoch   4 |  2800/ 2983 batches | lr 5.00 | ms/batch 51.91 | loss  7.01 | ppl  1102.79
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 163.09s | valid loss  6.99 | valid ppl  1085.24
-----------------------------------------------------------------------------------------
| epoch   5 |   200/ 2983 batches | lr 5.00 | ms/batch 51.60 | loss  7.03 | ppl  1130.36
| epoch   5 |   400/ 2983 batches | lr 5.00 | ms/batch 51.72 | loss  7.01 | ppl  1109.07
| epoch   5 |   600/ 2983 batches | lr 5.00 | ms/batch 51.85 | loss  6.99 | ppl  1089.05
| epoch   5 |   800/ 2983 batches | lr 5.00 | ms/batch 52.67 | loss  7.01 | ppl  1103.49
| epoch   5 |  1000/ 2983 batches | lr 5.00 | ms/batch 53.60 | loss  7.02 | ppl  1123.31
| epoch   5 |  1200/ 2983 batches | lr 5.00 | ms/batch 52.59 | loss  7.04 | ppl  1136.08
| epoch   5 |  1400/ 2983 batches | lr 5.00 | ms/batch 53.74 | loss  7.01 | ppl  1106.78
| epoch   5 |  1600/ 2983 batches | lr 5.00 | ms/batch 54.21 | loss  7.02 | ppl  1118.34
| epoch   5 |  1800/ 2983 batches | lr 5.00 | ms/batch 53.74 | loss  7.00 | ppl  1093.27
| epoch   5 |  2000/ 2983 batches | lr 5.00 | ms/batch 54.96 | loss  7.02 | ppl  1115.05
| epoch   5 |  2200/ 2983 batches | lr 5.00 | ms/batch 54.56 | loss  7.02 | ppl  1117.92
| epoch   5 |  2400/ 2983 batches | lr 5.00 | ms/batch 53.15 | loss  6.98 | ppl  1075.34
| epoch   5 |  2600/ 2983 batches | lr 5.00 | ms/batch 53.58 | loss  7.01 | ppl  1102.60
| epoch   5 |  2800/ 2983 batches | lr 5.00 | ms/batch 53.40 | loss  6.97 | ppl  1066.26
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 167.32s | valid loss  7.05 | valid ppl  1155.16
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  6.92 | test ppl  1013.72
=========================================================================================
----

=== Generate

[source, bash]
.*Generate 100 new words*
----
% python3 generate.py --mps --checkpoint model-transformer.pt --words 100 
50 calling = was stabilized fountains ) ( birds for gate Villiers . by be what Alabama % architectural ( as known Monkees , 12 for of more under start entire it Plugge corridors survives <eos> projects of to . visual Saprang were records different 5 of until . = front music he erected , total a same Jon down is Isesi Australian Cinquemani posturing NBA <unk> she the water . dreams jump Boom right of between . " , coded from writings a Jordan Star 2012 sensitive . primary with academic , pretty teaches order There , a the 
----

== Model Evaluation

=== Trained Model Evaluation

[source, bash]
.*RNN/LSTM Model*
----
% python3 evaluate.py --mps --model LSTM --saved model.pt
 test, loss  6.16, ppl   471.20
valid, loss  6.20, ppl   493.17

% python3 evaluate.py --mps --model LSTM --saved model-lstm.pt 
 test, loss  6.27, ppl   528.65
valid, loss  6.31, ppl   550.97

% python3 evaluate.py --mps --model LSTM --saved lstm.pt 
 test, loss  6.19, ppl   487.62
valid, loss  6.27, ppl   527.27
----

[source, bash]
.*Transformer*
----
% python3 evaluate.py --mps --model Transformer --saved model-transformer.pt 
 test, loss  6.92, ppl  1013.72
valid, loss  6.99, ppl  1085.24

% python3 evaluate.py --mps --model Transformer --saved transformer.pt
 test, loss  6.84, ppl   930.84
valid, loss  6.90, ppl   992.60
----
