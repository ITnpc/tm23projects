## 新闻分类
项目任务是对新闻文本进行聚类分析。技术路线为数据预处理以及训练，使用的技术包括CNN，bert模型。
输入是一组新闻文本，输出是该新闻对应的类别，如“军事”、“政治”等。
数据集为网上找到标准数据集。数据集有点大，放到了网盘中：链接: https://pan.baidu.com/s/1200sDOdgazSbn0ZjdtE9_g 提取码: 7x2f 

### 程序运行说明

##### 硬件

- GPU: Tesla V100, 32GB显存
- 内存：32GB
- 系统：Linux（训练），Windows（展示）

##### 软件

- CUDA版本： 9.2
- Pytorch：1.5
- 其他库：gensim，sklearn，tqdm，flask，numpy等

##### 运行方法

如果直接使用，步骤为：

1. 进入"Flask"文件夹，在cmd中执行以下命令：`python NLP_flask.py`，便可启动 flask 后台，然后在浏览器地址栏输入127.0.0.1:5000，即可看到分类系统界面。

如果需要训练，步骤为：

1. 如果想要训练非bert的模型，需要先训练词向量：进入"src"目录下，在terminal中执行以下命令：`python train_w2v.py`，修改该文件的代码可以设置word2vec的窗口大小、词向量维度等。
2. （./src目录）在terminal中输入`python run.py --model model_name --word True/False`，即可启动相应模型的训练。model_name是选择的模型，word为True（默认）则进行词级别的训练，否则进行字级别的训练。word参数只针对非bert模型，因为bert是分字的。可选的模型会在后面介绍。



##### 代码文件说明

下面解释./src目录下的代码：

- run.py 训练主程序
- train_eval.py 具体的训练逻辑
- utils.py 工具类函数
- train_w2v.py 训练词向量
- global_config.py 全局参数设置，如batch_size等
- ./model 下是不同模型的实现
- ./temp 下存放了预训练的词向量



#### 数据预处理



1. 分词

   采用jieba分词对数据预处理。首先使用jieba分词将所有的文章中的单词分开，单词与单词之间用空格分隔，并在最后一个单词后面加上退格符：\t，上述内容构成数据集中的一行数据。我们的对所有的文章进行遍历，按照上述内容构建我们的数据集，并划分出测试集和验证集，它们的比例为：0.9: 0.1: 0.1。


2. 填充/截断

   我们需要将每一个文本数据转换成一个等长的数据，以便神经网络处理。但新闻文本一般长度不一，所以我们需要对长的文本进行截断，对长度不够的文本进行填充。我们设定的长度值为500，即每篇文章都被处理成500个词的表示。

   对于bert模型，由于参数量过大，我们将长度值设为256。

#### 训练

在该系统中，我们使用了9个模型进行实验，它们分别是：bert、bert-LSTM、LSTM、LSTM-Attention、LSTM-GRU、FastText、TextCNN、TextRCNN、DPCNN，由于篇幅限制，下面仅对其进行简单的介绍。值得注意的是，我们在某些模型中加入了自己的想法，虽然只是少量的改动，但我们认为也是一种微创新。在FastText模型中，由于中文没有像英文这种字符级的n-gram，而1-gram就相当于中文字级别的建模了，所以我们同时使用了1-gram，2-gram和3-gram，将其拼在一起进行分类，效果比只用1-gram要提升不少。另外，我们也自己实现了一个模型：LSTM-GRU，它是将两种变种RNN堆叠起来实现的一个模型，实际效果显示也取得不错的精度。

##### bert 模型

关键模块：

1. Multi-head Self-Attention

作者用多个Self-Attention模块来表示不同的语义空间，从而获取文本中每个字在不同语义空间的词向量表示，然后将每个字的多个空间的词向量表示进行线性组合得到增强的语义表示向量。

2. 残差连接

这个步骤是将模型的输入向量与Multi-head Self-Attention的输出向量直接相加，作为最后的输出，这种残差连接有利解决梯度消失的问题，而且由于修改输入比重构整个输出更容易，所以模型也变得更加容易训练。

3. Layer Normalization ：对残差连接后的输出向量作均值为，方差为1的标准化。

4. 线性转换：对Layer Normalization后的增强语义向量进行两次线性变换，来增强整个模型的表达能力。


    


### 结论

在本项目中，利用深度学习方法进行了新闻文本的分类。我们先对数据进行预处理，包括清洗和训练词向量等，Bert模型分类效果较好。





