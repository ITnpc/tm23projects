# 基于hfl/chinese-roberta-wwm-ext模型的情感二分类任务

大数据科学与工程：肖佩； 学号：231017000009

## 1. 任务说明
使用Huggingface社区提供的dataset和model，完成情感二分类的任务。整个过程包括了使用一个相对较小的数据集先对约1亿个参数的模型进行预训练，并在此基础上锁定预训练模型参数，使用一个更大的数据集进行训练。在算力有限的情况下，同时训练backbone和下游模型。

## 2. 数据集

本次任务采用了以下两个数据集：

数据集1: Huggingface - [seamew/ChnSentiCorp](https://huggingface.co/datasets/seamew/ChnSentiCorp). 9600条数据用于预训练模型的训练；

数据集2: Huggingface - [t1annnnn/Chinese_sentimentAnalyze](https://huggingface.co/datasets/t1annnnn/Chinese_sentimentAnalyze).140000条数据用于下游任务模型的训练。


## 3. 模型

采用了Huggingface - [hfl/chinese-roberta-wwm-ext模型](https://arxiv.org/abs/1906.08101)进行数据集的特征抽取，以供下游模型计算二分类的结果并与数据集中的label进行比对。下游模型获取一批数据后，使用预训练模型抽取成特征矩阵，然后使用全连接线性神经网络输出二分类结果。

## 4. 训练
第一次训练使用了数据集1的9600条数据进行训练，同时训练预训练模型。训练过程分成600个batch，每10个批次输出训练情况。完成后手动保存预训练模型至本地。Script:_src/pretrained_model.py_.

第二次训练使用了数据集2的140000条数据进行训练，训练时加载已保存到本地的完成了预训练的预训练模型，锁定模型参数，不计算梯度。训练数据分成4320个batch，每60个批次输出训练情况。Script:_src/model_train.py_.

## 5. 测试
第一次训练结束后切换训练模式到运行模式，抽取1个batch的数据做测试并随机输出5条数据的结果:_Accuracy for the first 5 sentences: 0.9375_.
第二次训练结束后的测试结果:_Accuracy for the first 5 sentences: 0.8125_.

## 6.任务总结
本次任务使用了数据量更小但质量更高的[seamew/ChnSentiCorp](https://huggingface.co/datasets/seamew/ChnSentiCorp)数据集进行预训练模型训练，因此第一次的训练结果Accuracy比第二次的更高。数据量更大但数据质量相对更不稳定的[t1annnnn/Chinese_sentimentAnalyze](https://huggingface.co/datasets/t1annnnn/Chinese_sentimentAnalyze)对于整个模型的挑战更大，虽然Accuracy比第一次低，但在有限算力下，上述两步训练的方式比不训练预训练模型的方式效果更好。
