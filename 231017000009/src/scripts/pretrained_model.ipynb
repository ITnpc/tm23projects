{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa05a422-4a99-40c1-9f41-88a2e337c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "pretrained_model_name_or_path = r'C:\\Users\\peixi\\Downloads\\Huggingface\\model\\hflchinese-roberta-wwm-ext'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split, max_samples=None):\n",
    "        self.dataset = load_from_disk(r'C:\\Users\\peixi\\Downloads\\Huggingface\\Data\\ChnSentiCorp')[split]\n",
    "        if max_samples:\n",
    "            self.dataset = self.dataset.select(list(range(min(max_samples, len(self.dataset)))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        review = self.dataset[i]['text']\n",
    "        label = self.dataset[i]['label']\n",
    "        return review, label\n",
    "\n",
    "dataset = Dataset('train', 9600)\n",
    "len(dataset), dataset[20]\n",
    "\n",
    "#定义计算设备\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "\n",
    "# 数据整理函数\n",
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "    # 编码\n",
    "    data = tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents, truncation=True, padding='max_length', max_length=500, return_tensors='pt', return_length=True)\n",
    "    # input_ids：编码之后的数字\n",
    "    # attention_mask：补零的位置是0, 其他位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "    # 把数据移动到计算设备上\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    token_type_ids = token_type_ids.to(device)\n",
    "    labels = labels.to(device)\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cf950c3-c963-44cb-850e-57a759d4297e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c50ea494-efdb-4f7b-9c11-34dd059cceea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#加载模型\n",
    "from transformers import BertModel\n",
    "pretrained_model_name_or_path = r'C:\\Users\\peixi\\Downloads\\Huggingface\\model\\hflchinese-roberta-wwm-ext'\n",
    "pretrained = BertModel.from_pretrained(pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa261e8-5652-4639-bd49-a97ea98e3191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#设定计算设备\n",
    "pretrained.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66d9f732-b9b3-4fdc-8e34-f6d040cf168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 500, 768])\n"
     ]
    }
   ],
   "source": [
    "# 预训练模型试算\n",
    "for input_ids, attention_mask, token_type_ids, labels in loader:\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    token_type_ids = token_type_ids.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    out = pretrained(input_ids=input_ids,\n",
    "                     attention_mask=attention_mask,\n",
    "                     token_type_ids=token_type_ids)\n",
    "\n",
    "    print(out.last_hidden_state.shape)\n",
    "    break  # 退出循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36fe4839-e7e8-4dba-b80b-a56393bcb3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(in_features=768, out_features=2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        #使用预训练模型抽取数据特征\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             token_type_ids=token_type_ids)\n",
    "\n",
    "        #对抽取的特征只取第一个字的结果做分类即可\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "\n",
    "        out = out.softmax(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "#设定计算设备\n",
    "model.to(device)\n",
    "\n",
    "#试算\n",
    "model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ad113a5-f9c6-43ec-9efc-56a44d7d8862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6771042943000793 0.00049 0.4375\n",
      "10 0.4872928559780121 0.00040036567537489795 0.8125\n",
      "20 0.4319842755794525 0.00032712790615999613 0.875\n",
      "30 0.4958016574382782 0.00026728731649739405 0.75\n",
      "40 0.4494304656982422 0.00021839319793596584 0.875\n",
      "50 0.3881189227104187 0.00017844314324268721 0.9375\n",
      "60 0.46474751830101013 0.00014580103991913902 0.8125\n",
      "70 0.46824130415916443 0.00011913006493385416 0.875\n",
      "80 0.41674932837486267 9.733793654019992e-05 0.875\n",
      "90 0.47692564129829407 7.953218102554305e-05 0.75\n",
      "100 0.3346036970615387 6.498358238842894e-05 1.0\n",
      "110 0.4870145916938782 5.309631806372179e-05 0.875\n",
      "120 0.4098713994026184 4.3383557635719116e-05 0.875\n",
      "130 0.44377946853637695 3.544752521018468e-05 0.8125\n",
      "140 0.4476511478424072 2.8963208920702668e-05 0.875\n",
      "150 0.4154006838798523 2.366505040930896e-05 0.9375\n",
      "160 0.32949233055114746 1.933606916306936e-05 1.0\n",
      "170 0.5102009177207947 1.5798976305240818e-05 0.8125\n",
      "180 0.5902838110923767 1.2908913915672958e-05 0.6875\n",
      "190 0.44055911898612976 1.0547522526948616e-05 0.9375\n",
      "200 0.4395008385181427 8.618093836803534e-06 0.875\n",
      "210 0.42904573678970337 7.041610121257331e-06 0.9375\n",
      "220 0.4386994540691376 5.753508146783487e-06 0.8125\n",
      "230 0.42101845145225525 4.701035050942751e-06 0.8125\n",
      "240 0.5727000832557678 3.841087904350535e-06 0.8125\n",
      "250 0.4016334116458893 3.138448475509496e-06 0.9375\n",
      "260 0.3850304186344147 2.5643409051564857e-06 0.875\n",
      "270 0.46408355236053467 2.095253221192762e-06 0.8125\n",
      "280 0.5545252561569214 1.7119744305801442e-06 0.75\n",
      "290 0.5347813963890076 1.3988077533138277e-06 0.75\n",
      "300 0.42391237616539 1.1429277772961926e-06 0.875\n",
      "310 0.4068514108657837 9.338552070651448e-07 0.875\n",
      "320 0.4642565846443176 7.630276952632691e-07 0.8125\n",
      "330 0.48212990164756775 6.234491807016949e-07 0.8125\n",
      "340 0.4341813921928406 5.094033720276752e-07 0.875\n",
      "350 0.6116674542427063 4.1621964302063376e-07 0.75\n",
      "360 0.43900614976882935 3.4008175200460194e-07 0.8125\n",
      "370 0.5029240846633911 2.778715516816347e-07 0.875\n",
      "380 0.49740850925445557 2.2704128868671128e-07 0.875\n",
      "390 0.44438207149505615 1.8550926302661698e-07 0.8125\n",
      "400 0.38786980509757996 1.5157457424479813e-07 1.0\n",
      "410 0.4646373391151428 1.2384746283098206e-07 0.875\n",
      "420 0.3807823956012726 1.0119239408121162e-07 0.875\n",
      "430 0.4354707896709442 8.268155346760636e-08 0.875\n",
      "440 0.4460838735103607 6.75568489695999e-08 0.875\n",
      "450 0.4986487329006195 5.519886421206906e-08 0.8125\n",
      "460 0.42545562982559204 4.5101490918759815e-08 0.875\n",
      "470 0.4090903699398041 3.6851201779804284e-08 0.9375\n",
      "480 0.3953663408756256 3.0110114875404044e-08 0.9375\n",
      "490 0.5445654988288879 2.4602156076952858e-08 0.8125\n",
      "500 0.5306004881858826 2.0101752721281384e-08 0.75\n",
      "510 0.48260298371315 1.642459551933676e-08 0.8125\n",
      "520 0.4049970805644989 1.3420090362977112e-08 0.9375\n",
      "530 0.35452699661254883 1.0965190901562225e-08 1.0\n",
      "540 0.599078357219696 8.959359307997236e-09 0.6875\n",
      "550 0.4292724132537842 7.320448857699371e-09 0.875\n",
      "560 0.6649922728538513 5.9813396958371605e-09 0.625\n",
      "570 0.3824906647205353 4.887190014225574e-09 0.9375\n",
      "580 0.508561372756958 3.993190062716079e-09 0.75\n",
      "590 0.49049443006515503 3.2627270129788855e-09 0.8125\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def train():\n",
    "    # 定义优化器\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "    # 定义loss函数\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # 定义学习率调节器\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: 0.98 ** step)  # 使用LambdaLR作为学习率调节器\n",
    "\n",
    "    # 模型切换到训练模式\n",
    "    model.train()\n",
    "\n",
    "    # 按批次遍历训练集中的数据\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "\n",
    "        # 模型计算\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        # 计算loss并使用梯度下降法优化模型参数\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 输出各项数据的情况，便于观察\n",
    "        if i % 10 == 0:\n",
    "            out = out.argmax(dim=1)\n",
    "            accuracy = (out == labels).sum().item() / len(labels)\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            print(i, loss.item(), lr, accuracy)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69a8e01b-228c-4c15-8213-ec3ebc96e9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: 性 价 比 极 高 ， 我 在 苏 宁 买 4699 ， 东 东 才 4399. 功 能 很 全 ， 用 起 来 很 满 意 ， 够 用 了 。\n",
      "Prediction: 1\n",
      "True label: 1\n",
      "Review: 看 完 之 后 ， 非 常 喜 欢 作 者 细 腻 的 情 感 和 文 笔 ， 她 把 都 市 生 活 中 可 能 遇 到 的 很 多 情 况 在 小 说 主 人 公 身 上 再 现 了 ， 让 读 书 的 我 们 感 同 身 受 ， 并 且 针 对 这 些 问 题 ， 针 对 我 们 经 常 会 出 现 的 负 面 情 绪 找 到 内 心 深 处 最 根 本 的 原 因 ， 印 证 了 作 者 的 观 点 ： 当 外 境 有 任 何 东 西 触 动 你 的 时 候 ， 记 得 ， 要 忘 往 内 看 。 全 书 看 完 ， 我 的 心 情 非 常 平 静 ， 那 种 感 觉 我 不 知 道 要 怎 样 表 述 ， 好 像 有 种 淡 淡 的 甜 味 ， 很 淡 ， 不 腻 人 ， 很 美 好 的 感 觉 。\n",
      "Prediction: 1\n",
      "True label: 1\n",
      "Review: 说 实 话 ， 写 的 实 在 不 怎 么 样 ， 东 拼 西 凑 ， 主 人 公 那 样 的 心 理 素 质 还 当 心 理 师 ， 实 在 米 名 奇 妙\n",
      "Prediction: 0\n",
      "True label: 0\n",
      "Review: 优 点 不 用 说 了 ， 谁 让 咱 是 穷 人 价 格 是 很 吸 引 人 的 因 素 。 我 只 说 说 缺 点 ， 拿 到 手 后 发 现 风 扇 发 出 很 尖 锐 的 高 频 声 音 ， 安 静 的 时 候 很 是 不 爽 。 一 气 之 下 我 把 它 给 拆 了 ， 应 该 是 装 配 问 题 ， 风 扇 运 转 很 安 静 ， 是 其 他 地 方 和 它 发 生 了 共 振 或 摩 擦 ， 从 新 装 了 一 遍 现 在 好 了 。 底 部 外 壳 在 按 的 的 时 候 会 发 生 一 个 咔 的 声 音 ， 经 过 仔 细 观 察 ， 是 底 壳 和 板 子 边 缘 产 生 的 ， 稍 微 处 理 一 下 接 触 部 位 解 决 。\n",
      "Prediction: 0\n",
      "True label: 1\n",
      "Review: 非 常 差 ， 服 务 差 。 房 间 比 较 大 ， 但 也 很 差 ， 更 重 要 的 是 临 近 铁 路 ， 很 吵 ！ 难 怪 酒 店 生 意 不 好 ， 人 很 少 ！\n",
      "Prediction: 0\n",
      "True label: 0\n",
      "Accuracy for the first 5 sentences: 0.9375\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    #定义测试数据集加载器\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('test'),\n",
    "                                              batch_size=16,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    #下游任务模型切换到运行模式\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sentence_count = 0  # 用于记录句子数量\n",
    "\n",
    "    #增加输出前5句的结果并与真实的label进行比较\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader_test):\n",
    "\n",
    "        #计算\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        #统计正确率\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "        #输出前5句的结果并与真实的label进行比较\n",
    "        for j in range(len(input_ids)):  # 遍历当前batch的每个句子\n",
    "            # Decode input_ids\n",
    "            decoded_input_ids = tokenizer.decode(input_ids[j], skip_special_tokens=True)\n",
    "            print(\"Review:\", decoded_input_ids)  # 输出 input_ids 的 decode 结果\n",
    "            print(\"Prediction:\", out[j].item())  # 输出预测结果\n",
    "            print(\"True label:\", labels[j].item())  # 输出真实标签\n",
    "            sentence_count += 1  # 句子数量加1\n",
    "            if sentence_count == 5:  # 当输出了5条句子后，退出循环\n",
    "                break\n",
    "\n",
    "        if sentence_count == 5:  # 当输出了5条句子后，退出外层循环\n",
    "            break\n",
    "\n",
    "    print(\"Accuracy for the first 5 sentences:\", correct / total)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99fd7502-01c8-44e6-bcc0-0c3eb993b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#手动保存模型\n",
    "pretrained.save_pretrained(r'C:\\Users\\peixi\\Downloads\\Huggingface\\model\\hflchinese-roberta-wwm-ext\\trained')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
