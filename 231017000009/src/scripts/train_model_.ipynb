{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa05a422-4a99-40c1-9f41-88a2e337c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "pretrained_model_name_or_path = r'C:\\Users\\peixi\\Downloads\\Huggingface\\model\\hflchinese-roberta-wwm-ext'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
    "\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split, max_samples=None):\n",
    "        self.dataset = load_from_disk(r'c:\\users\\peixi\\downloads\\huggingface\\data\\Chnsentialz')[split]\n",
    "        if max_samples:\n",
    "            self.dataset = self.dataset.select(list(range(min(max_samples, len(self.dataset)))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        review = self.dataset[i]['review']\n",
    "        label = self.dataset[i]['label']\n",
    "        return review, label\n",
    "\n",
    "dataset = Dataset('train', 140000) \n",
    "len(dataset), dataset[20]\n",
    "\n",
    "#定义计算设备\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "\n",
    "# 数据整理函数\n",
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "    # 编码\n",
    "    data = tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents, truncation=True, padding='max_length', max_length=500, return_tensors='pt', return_length=True)\n",
    "    # input_ids：编码之后的数字\n",
    "    # attention_mask：补零的位置是0, 其他位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "    # 把数据移动到计算设备上\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    token_type_ids = token_type_ids.to(device)\n",
    "    labels = labels.to(device)\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cf950c3-c963-44cb-850e-57a759d4297e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4375"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=32,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c50ea494-efdb-4f7b-9c11-34dd059cceea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#加载模型\n",
    "from transformers import BertModel\n",
    "pretrained_model_name_or_path = r'C:\\Users\\peixi\\Downloads\\Huggingface\\model\\hflchinese-roberta-wwm-ext\\trained'\n",
    "pretrained = BertModel.from_pretrained(pretrained_model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37b90a07-01c4-42f9-a3a4-cadd2e6ad94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#不训练预训练模型,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aa261e8-5652-4639-bd49-a97ea98e3191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#设定计算设备\n",
    "pretrained.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d9f732-b9b3-4fdc-8e34-f6d040cf168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 500, 768])\n"
     ]
    }
   ],
   "source": [
    "# 预训练模型试算\n",
    "for input_ids, attention_mask, token_type_ids, labels in loader:\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    token_type_ids = token_type_ids.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    out = pretrained(input_ids=input_ids,\n",
    "                     attention_mask=attention_mask,\n",
    "                     token_type_ids=token_type_ids)\n",
    "\n",
    "    print(out.last_hidden_state.shape)\n",
    "    break  # 退出循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36fe4839-e7e8-4dba-b80b-a56393bcb3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(in_features=768, out_features=2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        #使用预训练模型抽取数据特征\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             token_type_ids=token_type_ids)\n",
    "\n",
    "        #对抽取的特征只取第一个字的结果做分类即可\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "\n",
    "        out = out.softmax(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "#设定计算设备\n",
    "model.to(device)\n",
    "\n",
    "#试算\n",
    "model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ad113a5-f9c6-43ec-9efc-56a44d7d8862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6955153942108154 0.00049 0.59375\n",
      "60 0.47216925024986267 0.00014580103991913902 0.84375\n",
      "120 0.5114313960075378 4.3383557635719116e-05 0.8125\n",
      "180 0.45744362473487854 1.2908913915672958e-05 0.875\n",
      "240 0.5374895930290222 3.841087904350535e-06 0.78125\n",
      "300 0.49398624897003174 1.1429277772961926e-06 0.8125\n",
      "360 0.503663957118988 3.4008175200460194e-07 0.84375\n",
      "420 0.501954197883606 1.0119239408121162e-07 0.8125\n",
      "480 0.6337202191352844 3.0110114875404044e-08 0.65625\n",
      "540 0.459847629070282 8.959359307997236e-09 0.84375\n",
      "600 0.5095941424369812 2.665885518602479e-09 0.71875\n",
      "660 0.4408225119113922 7.932426141175809e-10 0.875\n",
      "720 0.48663631081581116 2.3603183274799914e-10 0.75\n",
      "780 0.4138438403606415 7.023201360954808e-11 0.90625\n",
      "840 0.4768548309803009 2.0897756367116802e-11 0.8125\n",
      "900 0.5736762285232544 6.218193082249874e-12 0.71875\n",
      "960 0.3871147930622101 1.8502428934898528e-12 0.9375\n",
      "1020 0.4521421194076538 5.50545587701668e-13 0.84375\n",
      "1080 0.41976290941238403 1.638165698159117e-13 0.9375\n",
      "1140 0.5456175804138184 4.8744135173767676e-14 0.78125\n",
      "1200 0.5333734154701233 1.45039706087641e-14 0.78125\n",
      "1260 0.5918262600898743 4.315702036151904e-15 0.71875\n",
      "1320 0.5357189774513245 1.2841507037797821e-15 0.78125\n",
      "1380 0.4447757601737976 3.8210307759997237e-16 0.84375\n",
      "1440 0.4710630774497986 1.1369597157220293e-16 0.875\n",
      "1500 0.5501442551612854 3.3830593652742963e-17 0.75\n",
      "1560 0.4239279627799988 1.0066399460513771e-17 0.84375\n",
      "1620 0.6217480301856995 2.9952887950701383e-18 0.6875\n",
      "1680 0.44110384583473206 8.912575942436142e-19 0.875\n",
      "1740 0.5001047253608704 2.6519649811540607e-19 0.84375\n",
      "1800 0.5701249241828918 7.891005144518407e-20 0.71875\n",
      "1860 0.47533994913101196 2.347993379751142e-20 0.84375\n",
      "1920 0.38174116611480713 6.986528091652455e-21 0.96875\n",
      "1980 0.46165895462036133 2.0788633901779707e-21 0.8125\n",
      "2040 0.49149614572525024 6.185723349750509e-22 0.8125\n",
      "2100 0.5184013843536377 1.8405814225422942e-22 0.78125\n",
      "2160 0.5388030409812927 5.476707866581932e-23 0.8125\n",
      "2220 0.626563549041748 1.6296116373081118e-23 0.59375\n",
      "2280 0.43551453948020935 4.848960640486806e-24 0.875\n",
      "2340 0.47586873173713684 1.4428234773672464e-24 0.8125\n",
      "2400 0.430478036403656 4.293166600405976e-25 0.90625\n",
      "2460 0.4661771357059479 1.2774452140516449e-25 0.84375\n",
      "2520 0.5561493039131165 3.8010783805807536e-26 0.71875\n",
      "2580 0.5359265804290771 1.1310228177608791e-26 0.75\n",
      "2640 0.5384222269058228 3.3653939388124695e-27 0.75\n",
      "2700 0.5519446730613708 1.0013835428906638e-27 0.75\n",
      "2760 0.40688884258270264 2.979648202272868e-28 0.9375\n",
      "2820 0.5731282234191895 8.866036867032187e-29 0.71875\n",
      "2880 0.45933273434638977 2.638117133009628e-29 0.8125\n",
      "2940 0.5375306606292725 7.849800437169414e-30 0.75\n",
      "3000 0.4928932189941406 2.33573278958574e-30 0.8125\n",
      "3060 0.588870644569397 6.950046320302702e-31 0.6875\n",
      "3120 0.5870205163955688 2.068008124461876e-31 0.71875\n",
      "3180 0.5035058259963989 6.153423165464689e-32 0.8125\n",
      "3240 0.6236564517021179 1.830970401198514e-32 0.625\n",
      "3300 0.48438772559165955 5.448109970528704e-33 0.84375\n",
      "3360 0.4973320960998535 1.6211022434630913e-33 0.8125\n",
      "3420 0.49555593729019165 4.823640671676898e-34 0.84375\n",
      "3480 0.46281346678733826 1.4352894410749913e-34 0.84375\n",
      "3540 0.429862380027771 4.2707488386468067e-35 0.875\n",
      "3600 0.5315020680427551 1.2707747385880807e-35 0.75\n",
      "3660 0.40510281920433044 3.7812301712064114e-36 0.9375\n",
      "3720 0.4881463050842285 1.1251169206847323e-36 0.8125\n",
      "3780 0.4747685194015503 3.347820756458233e-37 0.8125\n",
      "3840 0.6031097173690796 9.961545872540591e-38 0.6875\n",
      "3900 0.4941481947898865 2.964089280446173e-38 0.75\n",
      "3960 0.4623052477836609 8.819740806167847e-39 0.8125\n",
      "4020 0.4641592502593994 2.624341594605179e-39 0.8125\n",
      "4080 0.4726051092147827 7.808810889724216e-40 0.84375\n",
      "4140 0.4421045780181885 2.323536220925893e-40 0.90625\n",
      "4200 0.406952440738678 6.913755046954725e-41 0.90625\n",
      "4260 0.4304083585739136 2.057209542024887e-41 0.90625\n",
      "4320 0.4846591055393219 6.121291644057228e-42 0.84375\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "def train():\n",
    "    # 定义优化器\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "    # 定义loss函数\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # 定义学习率调节器\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: 0.98 ** step)  # 使用LambdaLR作为学习率调节器\n",
    "\n",
    "    # 模型切换到训练模式\n",
    "    model.train()\n",
    "\n",
    "    # 按批次遍历训练集中的数据\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader):\n",
    "\n",
    "        # 模型计算\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        # 计算loss并使用梯度下降法优化模型参数\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 输出各项数据的情况，便于观察\n",
    "        if i % 60 == 0:\n",
    "            out = out.argmax(dim=1)\n",
    "            accuracy = (out == labels).sum().item() / len(labels)\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            print(i, loss.item(), lr, accuracy)\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69a8e01b-228c-4c15-8213-ec3ebc96e9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: 第 二 次 穿 裤 子 就 起 毛 了 看 来 是 买 到 假 劣 产 品 了\n",
      "Prediction: 0\n",
      "True label: 0\n",
      "Review: 我 买 浅 卡 色 和 宝 蓝 发 过 来 的 是 卡 其 色\n",
      "Prediction: 0\n",
      "True label: 0\n",
      "Review: 最 近 落 魄 了 很 久 真 的 是 失 魂 落 魄 银 行 用 普 通 白 信 封 寄 给 我 的 新 信 用 卡 都 被 我 忽 略 到 了 角 落 隔 了 二 十 多 天 才 好 容 易 从 一 厚 沓 邮 件 中 翻 出 来 我 真 是 该 打 闭 门 思 过 中\n",
      "Prediction: 0\n",
      "True label: 0\n",
      "Review: 质 量 不 错 面 料 挺 好 的 很 划 算\n",
      "Prediction: 1\n",
      "True label: 1\n",
      "Review: 双 鱼 又 开 始 多 愁 善 感 了\n",
      "Prediction: 0\n",
      "True label: 0\n",
      "Accuracy for the first 5 sentences: 0.8125\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    #定义测试数据集加载器\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('test'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    #下游任务模型切换到运行模式\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sentence_count = 0  # 用于记录句子数量\n",
    "\n",
    "    #增加输出前5句的结果并与真实的label进行比较\n",
    "    for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(loader_test):\n",
    "\n",
    "        #计算\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        #统计正确率\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "        #输出前5句的结果并与真实的label进行比较\n",
    "        for j in range(len(input_ids)):  # 遍历当前batch的每个句子\n",
    "            # Decode input_ids\n",
    "            decoded_input_ids = tokenizer.decode(input_ids[j], skip_special_tokens=True)\n",
    "            print(\"Review:\", decoded_input_ids)  # 输出 input_ids 的 decode 结果\n",
    "            print(\"Prediction:\", out[j].item())  # 输出预测结果\n",
    "            print(\"True label:\", labels[j].item())  # 输出真实标签\n",
    "            sentence_count += 1  # 句子数量加1\n",
    "            if sentence_count == 5:  # 当输出了5条句子后，退出循环\n",
    "                break\n",
    "\n",
    "        if sentence_count == 5:  # 当输出了5条句子后，退出外层循环\n",
    "            break\n",
    "\n",
    "    print(\"Accuracy for the first 5 sentences:\", correct / total)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4f43b-c63d-437f-8fa0-ba3e735f9089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
