{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d67f2468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#全局变量\n",
    "push_to_hub = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2e6ab7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445c2da7fc9b41108d2454580840afbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ccdbbaaf90246ffa93648aa1c7d3f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1027c76556554464aba639ea28f898f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d037cd0822434decbcda918e064ed705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7592, 1010, 2023, 2003, 2034, 6251, 3975, 2046, 2616, 1012, 102], [101, 2023, 2003, 2117, 6251, 3975, 2046, 2616, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载编码器\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased',\n",
    "                                          use_fast=True)\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "#编码试算\n",
    "tokenizer.batch_encode_plus([[\n",
    "    'Hello', ',', 'this', 'is', 'first', 'sentence', 'split', 'into', 'words',\n",
    "    '.'\n",
    "], ['This', 'is', 'second', 'sentence', 'split', 'into', 'words', '.']],\n",
    "                            is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4355f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (C:\\Users\\Administrator\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8621d57d85eb4e469aa63cd300223608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看数据样例\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "}) {'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14c10fbea9b41f5bdb462e1e13db788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d945277807544fa9a762ac0c647ee593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90008c808dd74edda70c97b3132b040c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "}) {'input_ids': [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    #加载数据\n",
    "    dataset = load_dataset(path='conll2003')\n",
    "\n",
    "    print('查看数据样例')\n",
    "    print(dataset, dataset['train'][0])\n",
    "\n",
    "    #根据以上说明性代码,写出这个数据处理函数\n",
    "    def tokenize_and_align_labels(data):\n",
    "        #分词\n",
    "        data_encode = tokenizer.batch_encode_plus(data['tokens'],\n",
    "                                                  truncation=True,\n",
    "                                                  is_split_into_words=True)\n",
    "\n",
    "        data_encode['labels'] = []\n",
    "        for i in range(len(data['tokens'])):\n",
    "            label = []\n",
    "            for word_id in data_encode.word_ids(batch_index=i):\n",
    "                if word_id is None:\n",
    "                    label.append(-100)\n",
    "                else:\n",
    "                    label.append(data['ner_tags'][i][word_id])\n",
    "\n",
    "            data_encode['labels'].append(label)\n",
    "\n",
    "        return data_encode\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=1,\n",
    "        remove_columns=['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "if push_to_hub:\n",
    "    dataset = get_dataset()\n",
    "    #dataset.push_to_hub(repo_id=repo_id, token=hub_token)\n",
    "\n",
    "#直接使用我处理好的数据集\n",
    "#dataset = load_dataset(path=repo_id)\n",
    "dataset.save_to_disk(dataset_dict_path='c:\\\\temp\\\\231017000028\\\\src\\\\dataset\\\\')\n",
    "print(dataset, dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26000c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([8, 54]) tensor([[  101, 14824,  1005,  1055,  7794,  7920,  2003,  2025,  2092,  2438,\n",
      "          2000,  5463,  2021,  1037,  4471,  2013,  2014,  2097,  2022,  3191,\n",
      "          2041,  2011,  1996, 19938,  1005,  1055,  2882,  1011,  2684, 17508,\n",
      "          6229,  2386,  2076,  1996,  3116,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  7367,  4244,  1010,  5479,  1011,  2039,  2000, 22160,  2197,\n",
      "          2095,  1010,  2003, 13916,  2000,  2448,  2046,  3587,  1011,  4396,\n",
      "          2446,  2019,  3489,  9594,  2121,  1999,  1996,  4284,  1011,  4399,\n",
      "          2007,  2959,  6534,  9530,  5428,  2696, 10337,  2030,  5964,  1011,\n",
      "         13916,  4386,  3410, 12110, 16273,  2559,  2066,  2014,  2087,  3497,\n",
      "         16797,  7892,  1012,   102]])\n",
      "attention_mask torch.Size([8, 54]) tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])\n",
      "labels torch.Size([8, 54]) tensor([[-100,    1,    0,    0,    0,    1,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    1,    2,    2,    0,    0,    0, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    1,    1,    0,    0,    0,    0,    0,    1,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    7,    1,    1,    2,\n",
      "            2,    0,    0,    0,    0,    0,    0,    0,    0,    1,    1,    1,\n",
      "            2,    0,    0,    0,    0,    7,    0,    1,    2,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, -100]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1755"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset['train'],\n",
    "    batch_size=8,\n",
    "    collate_fn=DataCollatorForTokenClassification(tokenizer),\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "for i, data in enumerate(loader):\n",
    "    break\n",
    "\n",
    "for k, v in data.items():\n",
    "    print(k, v.shape, v[:2])\n",
    "\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72a6db43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f926fc228d54d8082baeb6dce8d20bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6636.9801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(2.3947, grad_fn=<NllLossBackward0>), torch.Size([8, 54, 9]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, DistilBertModel, PreTrainedModel, PretrainedConfig\n",
    "\n",
    "#加载模型\n",
    "#model = AutoModelForTokenClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_name))\n",
    "\n",
    "#定义下游任务模型\n",
    "class Model(PreTrainedModel):\n",
    "    config_class = PretrainedConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.pretrained = DistilBertModel.from_pretrained(\n",
    "            'distilbert-base-uncased')\n",
    "\n",
    "        #9 = len(dataset['train'].features['ner_tags'].feature.names)\n",
    "        self.fc = torch.nn.Sequential(torch.nn.Dropout(0.1),\n",
    "                                      torch.nn.Linear(768, 9))\n",
    "\n",
    "        #加载预训练模型的参数\n",
    "        parameters = AutoModelForTokenClassification.from_pretrained(\n",
    "            'distilbert-base-uncased', num_labels=9)\n",
    "        self.fc[1].load_state_dict(parameters.classifier.state_dict())\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        logits = self.pretrained(input_ids=input_ids,\n",
    "                                 attention_mask=attention_mask)\n",
    "        logits = logits.last_hidden_state\n",
    "\n",
    "        logits = self.fc(logits)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits.flatten(end_dim=1), labels.flatten())\n",
    "\n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "\n",
    "model = Model(PretrainedConfig())\n",
    "\n",
    "#统计参数量\n",
    "print(sum(i.numel() for i in model.parameters()) / 10000)\n",
    "\n",
    "out = model(**data)\n",
    "\n",
    "out['loss'], out['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c924a315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "0.9699435028248587\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    #数据加载器\n",
    "    loader_test = torch.utils.data.DataLoader(\n",
    "        dataset=dataset['test'],\n",
    "        batch_size=16,\n",
    "        collate_fn=DataCollatorForTokenClassification(tokenizer),\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    outs = []\n",
    "    for i, data in enumerate(loader_test):\n",
    "        #计算\n",
    "        with torch.no_grad():\n",
    "            out = model(**data)\n",
    "\n",
    "        out = out['logits'].argmax(dim=2)\n",
    "\n",
    "        for j in range(16):\n",
    "            #使用attention_mask筛选label,很显然,不需要pad的预测结果\n",
    "            #另外首尾两个特殊符号也不需要预测结果\n",
    "            select = data['attention_mask'][j] == 1\n",
    "            labels.append(data['labels'][j][select][1:-1])\n",
    "            outs.append(out[j][select][1:-1])\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "\n",
    "        if i == 50:\n",
    "            break\n",
    "\n",
    "    #计算正确率\n",
    "    labels = torch.cat(labels)\n",
    "    outs = torch.cat(outs)\n",
    "\n",
    "    print((labels == outs).sum().item() / len(labels))\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1375fcad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.399038553237915 0.024390243902439025 1.998860398860399e-05\n",
      "50 0.5870899558067322 0.8269230769230769 1.941880341880342e-05\n",
      "100 0.51480633020401 0.8372093023255814 1.8849002849002852e-05\n",
      "150 0.13835808634757996 0.9583333333333334 1.827920227920228e-05\n",
      "200 0.13117648661136627 0.9669421487603306 1.770940170940171e-05\n",
      "250 0.23385891318321228 0.94 1.713960113960114e-05\n",
      "300 0.1472039669752121 0.9512195121951219 1.6569800569800573e-05\n",
      "350 0.19199655950069427 0.9505494505494505 1.6000000000000003e-05\n",
      "400 0.0964977890253067 0.9759036144578314 1.5430199430199432e-05\n",
      "450 0.5718852877616882 0.8689655172413793 1.4860398860398862e-05\n",
      "500 0.14928022027015686 0.9669421487603306 1.4290598290598293e-05\n",
      "550 0.04728260636329651 0.9913793103448276 1.3720797720797722e-05\n",
      "600 0.06350374221801758 0.9777777777777777 1.3150997150997152e-05\n",
      "650 0.17678901553153992 0.9712230215827338 1.2581196581196581e-05\n",
      "700 0.12549585103988647 0.96875 1.2011396011396012e-05\n",
      "750 0.06563395261764526 0.9820627802690582 1.1441595441595444e-05\n",
      "800 0.24276818335056305 0.917910447761194 1.0871794871794871e-05\n",
      "850 0.09486348181962967 0.9707602339181286 1.0301994301994303e-05\n",
      "900 0.08860410004854202 0.9617224880382775 9.732193732193734e-06\n",
      "950 0.08840496093034744 0.96875 9.162393162393163e-06\n",
      "1000 0.2855250835418701 0.9044117647058824 8.592592592592593e-06\n",
      "1050 0.06121617928147316 0.9864864864864865 8.022792022792024e-06\n",
      "1100 0.025660444051027298 1.0 7.452991452991454e-06\n",
      "1150 0.16198216378688812 0.9543147208121827 6.883190883190884e-06\n",
      "1200 0.2566322386264801 0.9340659340659341 6.313390313390314e-06\n",
      "1250 0.05306340008974075 0.9806763285024155 5.743589743589743e-06\n",
      "1300 0.11146940290927887 0.965034965034965 5.173789173789175e-06\n",
      "1350 0.014558409340679646 0.9921259842519685 4.603988603988604e-06\n",
      "1400 0.06073068827390671 0.9666666666666667 4.034188034188034e-06\n",
      "1450 0.060085318982601166 0.9767441860465116 3.4643874643874647e-06\n",
      "1500 0.036706432700157166 0.9879518072289156 2.894586894586895e-06\n",
      "1550 0.06373769044876099 0.9751243781094527 2.324786324786325e-06\n",
      "1600 0.019120031967759132 0.9933333333333333 1.7549857549857553e-06\n",
      "1650 0.04240876063704491 1.0 1.1851851851851854e-06\n",
      "1700 0.095241978764534 0.9797297297297297 6.153846153846155e-07\n",
      "1750 0.030119173228740692 0.9915966386554622 4.5584045584045586e-08\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_scheduler\n",
    "\n",
    "\n",
    "#训练\n",
    "def train():\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = get_scheduler(name='linear',\n",
    "                              num_warmup_steps=0,\n",
    "                              num_training_steps=len(loader),\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    for i, data in enumerate(loader):\n",
    "        for k in data.keys():\n",
    "            data[k] = data[k].to(device)\n",
    "            \n",
    "        out = model(**data)\n",
    "        loss = out['loss']\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            labels = []\n",
    "            outs = []\n",
    "            out = out['logits'].argmax(dim=2)\n",
    "            for j in range(8):\n",
    "                #使用attention_mask筛选label,很显然,不需要pad的预测结果\n",
    "                #另外首尾两个特殊符号也不需要预测结果\n",
    "                select = data['attention_mask'][j] == 1\n",
    "                labels.append(data['labels'][j][select][1:-1])\n",
    "                outs.append(out[j][select][1:-1])\n",
    "\n",
    "            #计算正确率\n",
    "            labels = torch.cat(labels)\n",
    "            outs = torch.cat(outs)\n",
    "            accuracy = (labels == outs).sum().item() / len(labels)\n",
    "\n",
    "            lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "\n",
    "            print(i, loss.item(), accuracy, lr)\n",
    "\n",
    "    model.to('cpu')\n",
    "\n",
    "\n",
    "if push_to_hub:\n",
    "    train()\n",
    "    #model.push_to_hub(repo_id=repo_id, use_auth_token=hub_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7652de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "0.9717212202182142\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
