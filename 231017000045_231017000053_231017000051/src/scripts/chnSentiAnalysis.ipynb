{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fe0e4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lenovo\\anaconda3_NEW\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1756: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from datasets import load_from_disk\n",
    "from transformers import BertTokenizer,BertConfig,AdamW,BertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 定义神经网络\n",
    "class BertClassificationModel(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(BertClassificationModel, self).__init__()   \n",
    "\t\t#加载预训练模型\n",
    "\t\tpretrained_weights=r\"E:\\model\\bert-base-chinese\"\n",
    "        #定义Bert模型\n",
    "\t\tself.bert = BertModel.from_pretrained(pretrained_weights)\n",
    "\t\tfor param in self.bert.parameters():\n",
    "\t\t\tparam.requires_grad = True\n",
    "\t\t#定义线性函数      \n",
    "\t\tself.dense = nn.Linear(768, 2)  #bert默认的隐藏单元数是768， 输出单元是2，表示二分类\n",
    "\tdef forward(self, input_ids,token_type_ids,attention_mask):\n",
    "\t\t#得到bert_output\n",
    "\t\tbert_output = self.bert(input_ids=input_ids,token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "\t\t#获得预训练模型的输出\n",
    "\t\tbert_cls_hidden_state = bert_output[1]\n",
    "\t\t#将768维的向量输入到线性层映射为二维向量\n",
    "\t\tlinear_output = self.dense(bert_cls_hidden_state)\n",
    "\t\treturn  linear_output\n",
    "\n",
    "# 使用BertTokenizer 编码成Bert需要的输入格式\n",
    "def encoder(max_len,vocab_path,text_list):\n",
    "\t#将text_list embedding成bert模型可用的输入形式\n",
    "\t#加载分词模型\n",
    "\ttokenizer = BertTokenizer.from_pretrained(vocab_path)\n",
    "\ttokenizer = tokenizer(\n",
    "\t\ttext_list,\n",
    "\t\tpadding = True,\n",
    "\t\ttruncation = True,\n",
    "\t\tmax_length = max_len,\n",
    "\t\treturn_tensors='pt'  # 返回的类型为pytorch tensor\n",
    "\t\t)\n",
    "\tinput_ids = tokenizer['input_ids']\n",
    "\ttoken_type_ids = tokenizer['token_type_ids']\n",
    "\tattention_mask = tokenizer['attention_mask']\n",
    "\treturn input_ids,token_type_ids,attention_mask\n",
    "# 将数据加载为Tensor格式\n",
    "def load_data(Dataset):\n",
    "\ttext_list = []\n",
    "\tlabels = []\n",
    "\tfor item in Dataset:\n",
    "\t\t#label在什么位置就改成对应的index\n",
    "\t\tlabel = int(item['label'])\n",
    "\t\ttext = item['text']\n",
    "\t\ttext_list.append(text)\n",
    "\t\tlabels.append(label)\n",
    "# 调用encoder函数，获得预训练模型的三种输入形式\n",
    "\tinput_ids,token_type_ids,attention_mask = encoder(max_len=150,vocab_path=r\"E:\\model\\bert-base-chinese\\vocab.txt\",text_list=text_list)\n",
    "\tlabels = torch.tensor(labels)\n",
    "\t#将encoder的返回值以及label封装为Tensor的形式\n",
    "\tdata = TensorDataset(input_ids,token_type_ids,attention_mask,labels)\n",
    "\treturn data\n",
    "\n",
    "#实例化DataLoader\n",
    "#设定batch_size\n",
    "batch_size = 16\n",
    "#从磁盘加载数据\n",
    "dataset = load_from_disk('E:\\datasets\\ChnSentiCorp')\n",
    "#取出训练集\n",
    "dataset_train = dataset['train']\n",
    "dataset_validation = dataset['validation']\n",
    "#调用load_data函数，将数据加载为Tensor形式\n",
    "dataset_train_ts = load_data(dataset_train)\n",
    "dataset_validation_ts = load_data(dataset_validation)\n",
    "#将训练数据和测试数据进行DataLoader实例化\n",
    "train_loader = DataLoader(dataset=dataset_train_ts, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(dataset=dataset_validation_ts, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 定义验证函数\n",
    "def dev(model,validation_loader):\n",
    "\t#将模型放到服务器上\n",
    "\tmodel.to(device)\n",
    "\t#设定模式为验证模式\n",
    "\tmodel.eval()\n",
    "\t#设定不会有梯度的改变仅作验证\n",
    "\twith torch.no_grad():\n",
    "\t\tcorrect = 0\n",
    "\t\ttotal = 0\n",
    "\t\tfor step, (input_ids,token_type_ids,attention_mask,labels) in tqdm(enumerate(validation_loader),desc='Dev Itreation:'):\n",
    "\t\t\tinput_ids,token_type_ids,attention_mask,labels=input_ids.to(device),token_type_ids.to(device),attention_mask.to(device),labels.to(device)\n",
    "\t\t\tout_put = model(input_ids,token_type_ids,attention_mask)\n",
    "\t\t\t_, predict = torch.max(out_put.data, 1)\n",
    "\t\t\tcorrect += (predict==labels).sum().item()\n",
    "\t\t\ttotal += labels.size(0)\n",
    "\t\tres = correct / total\n",
    "\t\treturn res\n",
    "\n",
    "# 定义训练函数 \n",
    "def train(model,train_loader,validation_loader):\n",
    "\t#将model放到服务器上\n",
    "\tmodel.to(device)\n",
    "\t#设定模型的模式为训练模式\n",
    "\tmodel.train()\n",
    "\t#定义模型的损失函数\n",
    "\tcriterion = nn.CrossEntropyLoss()\n",
    "\tparam_optimizer = list(model.named_parameters())\n",
    "\tno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\t#设置模型参数的权重衰减\n",
    "\toptimizer_grouped_parameters = [\n",
    "\t\t{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "\t\t'weight_decay': 0.01},\n",
    "\t\t{'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "\t]\n",
    "\t#学习率的设置\n",
    "\toptimizer_params = {'lr': 1e-5, 'eps': 1e-6, 'correct_bias': False}\n",
    "\t#使用AdamW 主流优化器\n",
    "\toptimizer = AdamW(optimizer_grouped_parameters, **optimizer_params)\n",
    "\t#学习率调整器，检测准确率的状态，然后衰减学习率\n",
    "\tscheduler = ReduceLROnPlateau(optimizer,mode='max',factor=0.5,min_lr=1e-7, patience=5,verbose= True, threshold=0.0001, eps=1e-08)\n",
    "\tt_total = len(train_loader)\n",
    "\t#设定训练轮次\n",
    "\ttotal_epochs = 2\n",
    "\tbestAcc = 0\n",
    "\tcorrect = 0\n",
    "\ttotal = 0\n",
    "\tprint('Training and verification begin!')\n",
    "\tfor epoch in range(total_epochs): \n",
    "\t\tfor step, (input_ids,token_type_ids,attention_mask,labels) in enumerate(train_loader):\n",
    "\t\t\t#从实例化的DataLoader中取出数据，并通过 .to(device)将数据部署到服务器上    input_ids,token_type_ids,attention_mask,labels=input_ids.to(device),token_type_ids.to(device),attention_mask.to(device),labels.to(device)\n",
    "\t\t\t#梯度清零\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\t#将数据输入到模型中获得输出\n",
    "\t\t\tout_put =  model(input_ids,token_type_ids,attention_mask)\n",
    "\t\t\t#计算损失\n",
    "\t\t\tloss = criterion(out_put, labels)\n",
    "\t\t\t_, predict = torch.max(out_put.data, 1)\n",
    "\t\t\tcorrect += (predict == labels).sum().item()\n",
    "\t\t\ttotal += labels.size(0)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\t#每两步进行一次打印\n",
    "\t\t\tif (step + 1) % 2 == 0:\n",
    "\t\t\t\ttrain_acc = correct / total\n",
    "\t\t\t\tprint(\"Train Epoch[{}/{}],step[{}/{}],tra_acc{:.6f} %,loss:{:.6f}\".format(epoch + 1, total_epochs, step + 1, len(train_loader),train_acc*100,loss.item()))\n",
    "\t\t\t#每五十次进行一次验证\n",
    "\t\t\tif (step + 1) % 50 == 0:\n",
    "\t\t\t\ttrain_acc = correct / total\n",
    "\t\t\t\t#调用验证函数dev对模型进行验证，并将有效果提升的模型进行保存\n",
    "\t\t\t\tacc = dev(model, validation_loader)\n",
    "\t\t\t\tif bestAcc < acc:\n",
    "\t\t\t\t\tbestAcc = acc\n",
    "\t\t\t\t\t#模型保存路径\n",
    "\t\t\t\t\tpath = r\"E:\\output\\savedmodel\\model_new.pkl\"\n",
    "\t\t\t\t\ttorch.save(model, path)\n",
    "\t\t\t\tprint(\"DEV Epoch[{}/{}],step[{}/{}],tra_acc:{:.6f} %,bestAcc{:.6f}%,dev_acc{:.6f} %,loss:{:.6f}\".format(epoch + 1, total_epochs, step + 1, len(train_loader),train_acc*100,bestAcc*100,acc*100,loss.item()))\n",
    "\t\tscheduler.step(bestAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fd9a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设备配置\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9da1104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#实例化模型\n",
    "model = BertClassificationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57803e23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lenovo\\anaconda3_NEW\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and verification begin!\n",
      "Train Epoch[1/2],step[2/600],tra_acc71.875000 %,loss:0.626144\n",
      "Train Epoch[1/2],step[4/600],tra_acc62.500000 %,loss:0.821733\n",
      "Train Epoch[1/2],step[6/600],tra_acc62.500000 %,loss:0.629073\n",
      "Train Epoch[1/2],step[8/600],tra_acc53.906250 %,loss:0.802826\n",
      "Train Epoch[1/2],step[10/600],tra_acc55.625000 %,loss:0.644059\n",
      "Train Epoch[1/2],step[12/600],tra_acc55.729167 %,loss:0.701151\n",
      "Train Epoch[1/2],step[14/600],tra_acc58.928571 %,loss:0.532708\n",
      "Train Epoch[1/2],step[16/600],tra_acc62.500000 %,loss:0.494226\n",
      "Train Epoch[1/2],step[18/600],tra_acc65.625000 %,loss:0.355855\n",
      "Train Epoch[1/2],step[20/600],tra_acc68.437500 %,loss:0.334875\n",
      "Train Epoch[1/2],step[22/600],tra_acc69.602273 %,loss:0.590296\n",
      "Train Epoch[1/2],step[24/600],tra_acc69.010417 %,loss:0.763688\n",
      "Train Epoch[1/2],step[26/600],tra_acc69.711538 %,loss:0.428184\n",
      "Train Epoch[1/2],step[28/600],tra_acc70.535714 %,loss:0.604404\n",
      "Train Epoch[1/2],step[30/600],tra_acc71.458333 %,loss:0.499765\n",
      "Train Epoch[1/2],step[32/600],tra_acc72.656250 %,loss:0.413947\n",
      "Train Epoch[1/2],step[34/600],tra_acc72.977941 %,loss:0.453068\n",
      "Train Epoch[1/2],step[36/600],tra_acc73.090278 %,loss:0.438881\n",
      "Train Epoch[1/2],step[38/600],tra_acc73.519737 %,loss:0.349038\n",
      "Train Epoch[1/2],step[40/600],tra_acc74.062500 %,loss:0.282040\n",
      "Train Epoch[1/2],step[42/600],tra_acc74.702381 %,loss:0.459887\n",
      "Train Epoch[1/2],step[44/600],tra_acc74.573864 %,loss:0.509447\n",
      "Train Epoch[1/2],step[46/600],tra_acc74.728261 %,loss:0.330472\n",
      "Train Epoch[1/2],step[48/600],tra_acc75.260417 %,loss:0.366980\n",
      "Train Epoch[1/2],step[50/600],tra_acc75.875000 %,loss:0.334971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [04:13,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[1/2],step[50/600],tra_acc:75.875000 %,bestAcc83.416667%,dev_acc83.416667 %,loss:0.334971\n",
      "Train Epoch[1/2],step[52/600],tra_acc76.562500 %,loss:0.228233\n",
      "Train Epoch[1/2],step[54/600],tra_acc76.851852 %,loss:0.172206\n",
      "Train Epoch[1/2],step[56/600],tra_acc77.120536 %,loss:0.487175\n",
      "Train Epoch[1/2],step[58/600],tra_acc77.478448 %,loss:0.319302\n",
      "Train Epoch[1/2],step[60/600],tra_acc77.708333 %,loss:0.241817\n",
      "Train Epoch[1/2],step[62/600],tra_acc78.024194 %,loss:0.419322\n",
      "Train Epoch[1/2],step[64/600],tra_acc78.320312 %,loss:0.311719\n",
      "Train Epoch[1/2],step[66/600],tra_acc78.977273 %,loss:0.098924\n",
      "Train Epoch[1/2],step[68/600],tra_acc79.503676 %,loss:0.298042\n",
      "Train Epoch[1/2],step[70/600],tra_acc79.910714 %,loss:0.130378\n",
      "Train Epoch[1/2],step[72/600],tra_acc80.295139 %,loss:0.177881\n",
      "Train Epoch[1/2],step[74/600],tra_acc80.743243 %,loss:0.027336\n",
      "Train Epoch[1/2],step[76/600],tra_acc80.921053 %,loss:0.192691\n",
      "Train Epoch[1/2],step[78/600],tra_acc80.769231 %,loss:0.922562\n",
      "Train Epoch[1/2],step[80/600],tra_acc80.781250 %,loss:0.206627\n",
      "Train Epoch[1/2],step[82/600],tra_acc80.945122 %,loss:0.332118\n",
      "Train Epoch[1/2],step[84/600],tra_acc81.175595 %,loss:0.297473\n",
      "Train Epoch[1/2],step[86/600],tra_acc81.322674 %,loss:0.261796\n",
      "Train Epoch[1/2],step[88/600],tra_acc81.392045 %,loss:0.260654\n",
      "Train Epoch[1/2],step[90/600],tra_acc81.597222 %,loss:0.134591\n",
      "Train Epoch[1/2],step[92/600],tra_acc81.861413 %,loss:0.185133\n",
      "Train Epoch[1/2],step[94/600],tra_acc81.981383 %,loss:0.320130\n",
      "Train Epoch[1/2],step[96/600],tra_acc82.161458 %,loss:0.320956\n",
      "Train Epoch[1/2],step[98/600],tra_acc82.206633 %,loss:0.259546\n",
      "Train Epoch[1/2],step[100/600],tra_acc82.375000 %,loss:0.354603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [04:12,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[1/2],step[100/600],tra_acc:82.375000 %,bestAcc87.500000%,dev_acc87.500000 %,loss:0.354603\n",
      "Train Epoch[1/2],step[102/600],tra_acc82.414216 %,loss:0.435248\n",
      "Train Epoch[1/2],step[104/600],tra_acc82.692308 %,loss:0.144677\n",
      "Train Epoch[1/2],step[106/600],tra_acc82.783019 %,loss:0.411722\n",
      "Train Epoch[1/2],step[108/600],tra_acc82.754630 %,loss:0.132808\n",
      "Train Epoch[1/2],step[110/600],tra_acc82.954545 %,loss:0.212569\n",
      "Train Epoch[1/2],step[112/600],tra_acc83.035714 %,loss:0.247810\n",
      "Train Epoch[1/2],step[114/600],tra_acc83.333333 %,loss:0.153866\n",
      "Train Epoch[1/2],step[116/600],tra_acc83.405172 %,loss:0.476932\n",
      "Train Epoch[1/2],step[118/600],tra_acc83.474576 %,loss:0.117877\n",
      "Train Epoch[1/2],step[120/600],tra_acc83.541667 %,loss:0.258853\n",
      "Train Epoch[1/2],step[122/600],tra_acc83.452869 %,loss:0.564014\n",
      "Train Epoch[1/2],step[124/600],tra_acc83.568548 %,loss:0.362664\n",
      "Train Epoch[1/2],step[126/600],tra_acc83.779762 %,loss:0.180259\n",
      "Train Epoch[1/2],step[128/600],tra_acc83.984375 %,loss:0.181857\n",
      "Train Epoch[1/2],step[130/600],tra_acc84.038462 %,loss:0.479513\n",
      "Train Epoch[1/2],step[132/600],tra_acc84.043561 %,loss:0.685167\n",
      "Train Epoch[1/2],step[134/600],tra_acc84.188433 %,loss:0.212491\n",
      "Train Epoch[1/2],step[136/600],tra_acc84.191176 %,loss:0.192642\n",
      "Train Epoch[1/2],step[138/600],tra_acc84.420290 %,loss:0.101910\n",
      "Train Epoch[1/2],step[140/600],tra_acc84.464286 %,loss:0.162405\n",
      "Train Epoch[1/2],step[142/600],tra_acc84.639085 %,loss:0.083836\n",
      "Train Epoch[1/2],step[144/600],tra_acc84.809028 %,loss:0.075462\n",
      "Train Epoch[1/2],step[146/600],tra_acc84.845890 %,loss:0.200740\n",
      "Train Epoch[1/2],step[148/600],tra_acc84.797297 %,loss:0.275665\n",
      "Train Epoch[1/2],step[150/600],tra_acc84.791667 %,loss:0.263572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [04:11,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[1/2],step[150/600],tra_acc:84.791667 %,bestAcc89.000000%,dev_acc89.000000 %,loss:0.263572\n",
      "Train Epoch[1/2],step[152/600],tra_acc84.868421 %,loss:0.332283\n",
      "Train Epoch[1/2],step[154/600],tra_acc84.983766 %,loss:0.095542\n",
      "Train Epoch[1/2],step[156/600],tra_acc85.096154 %,loss:0.298339\n",
      "Train Epoch[1/2],step[158/600],tra_acc85.166139 %,loss:0.119378\n",
      "Train Epoch[1/2],step[160/600],tra_acc85.312500 %,loss:0.162254\n",
      "Train Epoch[1/2],step[162/600],tra_acc85.262346 %,loss:0.401413\n",
      "Train Epoch[1/2],step[164/600],tra_acc85.251524 %,loss:0.306531\n",
      "Train Epoch[1/2],step[166/600],tra_acc85.353916 %,loss:0.156823\n",
      "Train Epoch[1/2],step[168/600],tra_acc85.416667 %,loss:0.403808\n",
      "Train Epoch[1/2],step[170/600],tra_acc85.257353 %,loss:0.572626\n",
      "Train Epoch[1/2],step[172/600],tra_acc85.174419 %,loss:0.580039\n",
      "Train Epoch[1/2],step[174/600],tra_acc85.308908 %,loss:0.152496\n",
      "Train Epoch[1/2],step[176/600],tra_acc85.404830 %,loss:0.240043\n",
      "Train Epoch[1/2],step[178/600],tra_acc85.428371 %,loss:0.284264\n",
      "Train Epoch[1/2],step[180/600],tra_acc85.486111 %,loss:0.293860\n",
      "Train Epoch[1/2],step[182/600],tra_acc85.473901 %,loss:0.331179\n",
      "Train Epoch[1/2],step[184/600],tra_acc85.461957 %,loss:0.326187\n",
      "Train Epoch[1/2],step[186/600],tra_acc85.483871 %,loss:0.318022\n",
      "Train Epoch[1/2],step[188/600],tra_acc85.538564 %,loss:0.245880\n",
      "Train Epoch[1/2],step[190/600],tra_acc85.526316 %,loss:0.397959\n",
      "Train Epoch[1/2],step[192/600],tra_acc85.546875 %,loss:0.181798\n",
      "Train Epoch[1/2],step[194/600],tra_acc85.534794 %,loss:0.250608\n",
      "Train Epoch[1/2],step[196/600],tra_acc85.682398 %,loss:0.118641\n",
      "Train Epoch[1/2],step[198/600],tra_acc85.763889 %,loss:0.196266\n",
      "Train Epoch[1/2],step[200/600],tra_acc85.781250 %,loss:0.092837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [04:11,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[1/2],step[200/600],tra_acc:85.781250 %,bestAcc89.500000%,dev_acc89.500000 %,loss:0.092837\n",
      "Train Epoch[1/2],step[202/600],tra_acc85.829208 %,loss:0.334193\n",
      "Train Epoch[1/2],step[204/600],tra_acc85.906863 %,loss:0.468750\n",
      "Train Epoch[1/2],step[206/600],tra_acc85.891990 %,loss:0.468275\n",
      "Train Epoch[1/2],step[208/600],tra_acc85.967548 %,loss:0.167955\n",
      "Train Epoch[1/2],step[210/600],tra_acc85.982143 %,loss:0.412424\n",
      "Train Epoch[1/2],step[212/600],tra_acc85.996462 %,loss:0.261865\n",
      "Train Epoch[1/2],step[214/600],tra_acc85.952103 %,loss:0.304708\n",
      "Train Epoch[1/2],step[216/600],tra_acc85.966435 %,loss:0.172056\n",
      "Train Epoch[1/2],step[218/600],tra_acc85.951835 %,loss:0.479677\n",
      "Train Epoch[1/2],step[220/600],tra_acc85.994318 %,loss:0.105435\n",
      "Train Epoch[1/2],step[222/600],tra_acc86.036036 %,loss:0.292577\n",
      "Train Epoch[1/2],step[224/600],tra_acc86.104911 %,loss:0.121126\n",
      "Train Epoch[1/2],step[226/600],tra_acc86.172566 %,loss:0.140068\n",
      "Train Epoch[1/2],step[228/600],tra_acc86.239035 %,loss:0.143164\n",
      "Train Epoch[1/2],step[230/600],tra_acc86.331522 %,loss:0.132986\n",
      "Train Epoch[1/2],step[232/600],tra_acc86.395474 %,loss:0.239254\n",
      "Train Epoch[1/2],step[234/600],tra_acc86.431624 %,loss:0.166441\n",
      "Train Epoch[1/2],step[236/600],tra_acc86.467161 %,loss:0.323942\n",
      "Train Epoch[1/2],step[238/600],tra_acc86.528361 %,loss:0.345196\n",
      "Train Epoch[1/2],step[240/600],tra_acc86.562500 %,loss:0.344786\n",
      "Train Epoch[1/2],step[242/600],tra_acc86.570248 %,loss:0.314012\n",
      "Train Epoch[1/2],step[244/600],tra_acc86.629098 %,loss:0.661715\n",
      "Train Epoch[1/2],step[246/600],tra_acc86.661585 %,loss:0.216146\n",
      "Train Epoch[1/2],step[248/600],tra_acc86.718750 %,loss:0.197437\n",
      "Train Epoch[1/2],step[250/600],tra_acc86.775000 %,loss:0.198803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [04:11,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[1/2],step[250/600],tra_acc:86.775000 %,bestAcc89.916667%,dev_acc89.916667 %,loss:0.198803\n",
      "Train Epoch[1/2],step[252/600],tra_acc86.830357 %,loss:0.157916\n",
      "Train Epoch[1/2],step[254/600],tra_acc86.860236 %,loss:0.348669\n",
      "Train Epoch[1/2],step[256/600],tra_acc86.840820 %,loss:0.382891\n",
      "Train Epoch[1/2],step[258/600],tra_acc86.845930 %,loss:0.337985\n",
      "Train Epoch[1/2],step[260/600],tra_acc86.899038 %,loss:0.358330\n",
      "Train Epoch[1/2],step[262/600],tra_acc86.951336 %,loss:0.075473\n",
      "Train Epoch[1/2],step[264/600],tra_acc86.979167 %,loss:0.306531\n",
      "Train Epoch[1/2],step[266/600],tra_acc87.006579 %,loss:0.340688\n",
      "Train Epoch[1/2],step[268/600],tra_acc87.080224 %,loss:0.295960\n",
      "Train Epoch[1/2],step[270/600],tra_acc87.037037 %,loss:0.629853\n",
      "Train Epoch[1/2],step[272/600],tra_acc87.040441 %,loss:0.228393\n",
      "Train Epoch[1/2],step[274/600],tra_acc87.066606 %,loss:0.161662\n",
      "Train Epoch[1/2],step[276/600],tra_acc87.092391 %,loss:0.174432\n",
      "Train Epoch[1/2],step[278/600],tra_acc87.072842 %,loss:0.167732\n",
      "Train Epoch[1/2],step[280/600],tra_acc87.120536 %,loss:0.197935\n",
      "Train Epoch[1/2],step[282/600],tra_acc87.167553 %,loss:0.128929\n",
      "Train Epoch[1/2],step[284/600],tra_acc87.147887 %,loss:0.373242\n",
      "Train Epoch[1/2],step[286/600],tra_acc87.150350 %,loss:0.053868\n",
      "Train Epoch[1/2],step[288/600],tra_acc87.152778 %,loss:0.176763\n",
      "Train Epoch[1/2],step[290/600],tra_acc87.241379 %,loss:0.069778\n",
      "Train Epoch[1/2],step[292/600],tra_acc87.221747 %,loss:0.232052\n",
      "Train Epoch[1/2],step[294/600],tra_acc87.266156 %,loss:0.228333\n",
      "Train Epoch[1/2],step[296/600],tra_acc87.309966 %,loss:0.097467\n",
      "Train Epoch[1/2],step[298/600],tra_acc87.311242 %,loss:0.299191\n",
      "Train Epoch[1/2],step[300/600],tra_acc87.312500 %,loss:0.371084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [04:08,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[1/2],step[300/600],tra_acc:87.312500 %,bestAcc89.916667%,dev_acc87.833333 %,loss:0.371084\n",
      "Train Epoch[1/2],step[302/600],tra_acc87.334437 %,loss:0.413331\n",
      "Train Epoch[1/2],step[304/600],tra_acc87.376645 %,loss:0.236385\n",
      "Train Epoch[1/2],step[306/600],tra_acc87.377451 %,loss:0.401605\n",
      "Train Epoch[1/2],step[308/600],tra_acc87.418831 %,loss:0.070097\n",
      "Train Epoch[1/2],step[310/600],tra_acc87.459677 %,loss:0.059098\n",
      "Train Epoch[1/2],step[312/600],tra_acc87.479968 %,loss:0.425469\n",
      "Train Epoch[1/2],step[314/600],tra_acc87.539809 %,loss:0.169715\n",
      "Train Epoch[1/2],step[316/600],tra_acc87.579114 %,loss:0.267189\n",
      "Train Epoch[1/2],step[318/600],tra_acc87.598270 %,loss:0.093757\n",
      "Train Epoch[1/2],step[320/600],tra_acc87.617188 %,loss:0.087927\n",
      "Train Epoch[1/2],step[322/600],tra_acc87.616460 %,loss:0.467100\n",
      "Train Epoch[1/2],step[324/600],tra_acc87.615741 %,loss:0.364666\n",
      "Train Epoch[1/2],step[326/600],tra_acc87.672546 %,loss:0.201078\n",
      "Train Epoch[1/2],step[328/600],tra_acc87.690549 %,loss:0.235306\n",
      "Train Epoch[1/2],step[330/600],tra_acc87.689394 %,loss:0.347627\n",
      "Train Epoch[1/2],step[332/600],tra_acc87.688253 %,loss:0.156872\n",
      "Train Epoch[1/2],step[334/600],tra_acc87.724551 %,loss:0.268243\n",
      "Train Epoch[1/2],step[336/600],tra_acc87.723214 %,loss:0.331522\n",
      "Train Epoch[1/2],step[338/600],tra_acc87.740385 %,loss:0.195540\n",
      "Train Epoch[1/2],step[340/600],tra_acc87.794118 %,loss:0.135976\n",
      "Train Epoch[1/2],step[342/600],tra_acc87.755848 %,loss:0.540025\n",
      "Train Epoch[1/2],step[344/600],tra_acc87.790698 %,loss:0.156733\n",
      "Train Epoch[1/2],step[346/600],tra_acc87.825145 %,loss:0.177395\n",
      "Train Epoch[1/2],step[348/600],tra_acc87.823276 %,loss:0.340973\n",
      "Train Epoch[1/2],step[350/600],tra_acc87.875000 %,loss:0.250373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [04:07,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[1/2],step[350/600],tra_acc:87.875000 %,bestAcc90.500000%,dev_acc90.500000 %,loss:0.250373\n",
      "Train Epoch[1/2],step[352/600],tra_acc87.872869 %,loss:0.229697\n",
      "Train Epoch[1/2],step[354/600],tra_acc87.888418 %,loss:0.313238\n",
      "Train Epoch[1/2],step[356/600],tra_acc87.938904 %,loss:0.050853\n",
      "Train Epoch[1/2],step[358/600],tra_acc87.971369 %,loss:0.060678\n",
      "Train Epoch[1/2],step[360/600],tra_acc87.968750 %,loss:0.331945\n",
      "Train Epoch[1/2],step[362/600],tra_acc87.966160 %,loss:0.095914\n",
      "Train Epoch[1/2],step[364/600],tra_acc87.980769 %,loss:0.312719\n",
      "Train Epoch[1/2],step[366/600],tra_acc87.995219 %,loss:0.202674\n",
      "Train Epoch[1/2],step[368/600],tra_acc88.026495 %,loss:0.314659\n",
      "Train Epoch[1/2],step[370/600],tra_acc88.074324 %,loss:0.153767\n",
      "Train Epoch[1/2],step[372/600],tra_acc88.121640 %,loss:0.063775\n",
      "Train Epoch[1/2],step[374/600],tra_acc88.151738 %,loss:0.358274\n",
      "Train Epoch[1/2],step[376/600],tra_acc88.148271 %,loss:0.504370\n",
      "Train Epoch[1/2],step[378/600],tra_acc88.210979 %,loss:0.085268\n",
      "Train Epoch[1/2],step[380/600],tra_acc88.240132 %,loss:0.056221\n",
      "Train Epoch[1/2],step[382/600],tra_acc88.268979 %,loss:0.396041\n",
      "Train Epoch[1/2],step[384/600],tra_acc88.264974 %,loss:0.281315\n",
      "Train Epoch[1/2],step[386/600],tra_acc88.277202 %,loss:0.317809\n",
      "Train Epoch[1/2],step[388/600],tra_acc88.289304 %,loss:0.249775\n",
      "Train Epoch[1/2],step[390/600],tra_acc88.317308 %,loss:0.042533\n",
      "Train Epoch[1/2],step[392/600],tra_acc88.329082 %,loss:0.174997\n",
      "Train Epoch[1/2],step[394/600],tra_acc88.356599 %,loss:0.225467\n",
      "Train Epoch[1/2],step[396/600],tra_acc88.383838 %,loss:0.075981\n",
      "Train Epoch[1/2],step[398/600],tra_acc88.395101 %,loss:0.180277\n",
      "Train Epoch[1/2],step[400/600],tra_acc88.406250 %,loss:0.397944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [03:59,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[1/2],step[400/600],tra_acc:88.406250 %,bestAcc90.500000%,dev_acc90.333333 %,loss:0.397944\n",
      "Train Epoch[1/2],step[402/600],tra_acc88.417289 %,loss:0.084185\n",
      "Train Epoch[1/2],step[404/600],tra_acc88.443688 %,loss:0.193778\n",
      "Train Epoch[1/2],step[406/600],tra_acc88.454433 %,loss:0.126475\n",
      "Train Epoch[1/2],step[408/600],tra_acc88.449755 %,loss:0.221054\n",
      "Train Epoch[1/2],step[410/600],tra_acc88.475610 %,loss:0.071736\n",
      "Train Epoch[1/2],step[412/600],tra_acc88.486044 %,loss:0.139626\n",
      "Train Epoch[1/2],step[414/600],tra_acc88.466184 %,loss:0.454997\n",
      "Train Epoch[1/2],step[416/600],tra_acc88.491587 %,loss:0.260643\n",
      "Train Epoch[1/2],step[418/600],tra_acc88.486842 %,loss:0.356250\n",
      "Train Epoch[1/2],step[420/600],tra_acc88.511905 %,loss:0.348447\n",
      "Train Epoch[1/2],step[422/600],tra_acc88.551540 %,loss:0.079849\n",
      "Train Epoch[1/2],step[424/600],tra_acc88.590802 %,loss:0.152501\n",
      "Train Epoch[1/2],step[426/600],tra_acc88.615023 %,loss:0.284456\n",
      "Train Epoch[1/2],step[428/600],tra_acc88.609813 %,loss:0.272826\n",
      "Train Epoch[1/2],step[430/600],tra_acc88.648256 %,loss:0.105700\n",
      "Train Epoch[1/2],step[432/600],tra_acc88.671875 %,loss:0.191458\n",
      "Train Epoch[1/2],step[434/600],tra_acc88.709677 %,loss:0.064885\n",
      "Train Epoch[1/2],step[436/600],tra_acc88.747133 %,loss:0.118286\n",
      "Train Epoch[1/2],step[438/600],tra_acc88.727169 %,loss:0.430077\n",
      "Train Epoch[1/2],step[440/600],tra_acc88.750000 %,loss:0.133046\n",
      "Train Epoch[1/2],step[442/600],tra_acc88.772624 %,loss:0.076786\n",
      "Train Epoch[1/2],step[444/600],tra_acc88.795045 %,loss:0.143485\n",
      "Train Epoch[1/2],step[446/600],tra_acc88.803251 %,loss:0.361461\n",
      "Train Epoch[1/2],step[448/600],tra_acc88.783482 %,loss:0.318461\n",
      "Train Epoch[1/2],step[450/600],tra_acc88.819444 %,loss:0.133069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [04:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[1/2],step[450/600],tra_acc:88.819444 %,bestAcc91.833333%,dev_acc91.833333 %,loss:0.133069\n",
      "Train Epoch[1/2],step[452/600],tra_acc88.813606 %,loss:0.453514\n",
      "Train Epoch[1/2],step[454/600],tra_acc88.794053 %,loss:0.426684\n",
      "Train Epoch[1/2],step[456/600],tra_acc88.774671 %,loss:0.159064\n",
      "Train Epoch[1/2],step[458/600],tra_acc88.810044 %,loss:0.086221\n",
      "Train Epoch[1/2],step[460/600],tra_acc88.845109 %,loss:0.086737\n",
      "Train Epoch[1/2],step[462/600],tra_acc88.866342 %,loss:0.065203\n",
      "Train Epoch[1/2],step[464/600],tra_acc88.900862 %,loss:0.158056\n",
      "Train Epoch[1/2],step[466/600],tra_acc88.921674 %,loss:0.230441\n",
      "Train Epoch[1/2],step[468/600],tra_acc88.955662 %,loss:0.134935\n",
      "Train Epoch[1/2],step[470/600],tra_acc89.002660 %,loss:0.085640\n",
      "Train Epoch[1/2],step[472/600],tra_acc88.996292 %,loss:0.503653\n",
      "Train Epoch[1/2],step[474/600],tra_acc88.989979 %,loss:0.442141\n",
      "Train Epoch[1/2],step[476/600],tra_acc89.009979 %,loss:0.399784\n",
      "Train Epoch[1/2],step[478/600],tra_acc89.029812 %,loss:0.090952\n",
      "Train Epoch[1/2],step[480/600],tra_acc88.984375 %,loss:0.400693\n",
      "Train Epoch[1/2],step[482/600],tra_acc88.978216 %,loss:0.270171\n",
      "Train Epoch[1/2],step[484/600],tra_acc89.023760 %,loss:0.165629\n",
      "Train Epoch[1/2],step[486/600],tra_acc89.043210 %,loss:0.213845\n",
      "Train Epoch[1/2],step[488/600],tra_acc89.075307 %,loss:0.054638\n",
      "Train Epoch[1/2],step[490/600],tra_acc89.094388 %,loss:0.067208\n",
      "Train Epoch[1/2],step[492/600],tra_acc89.126016 %,loss:0.046590\n",
      "Train Epoch[1/2],step[494/600],tra_acc89.132085 %,loss:0.162814\n",
      "Train Epoch[1/2],step[496/600],tra_acc89.125504 %,loss:0.541145\n",
      "Train Epoch[1/2],step[498/600],tra_acc89.144076 %,loss:0.058587\n",
      "Train Epoch[1/2],step[500/600],tra_acc89.162500 %,loss:0.067616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [03:57,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[1/2],step[500/600],tra_acc:89.162500 %,bestAcc92.166667%,dev_acc92.166667 %,loss:0.067616\n",
      "Train Epoch[1/2],step[502/600],tra_acc89.143426 %,loss:0.346785\n",
      "Train Epoch[1/2],step[504/600],tra_acc89.149306 %,loss:0.404289\n",
      "Train Epoch[1/2],step[506/600],tra_acc89.155138 %,loss:0.136020\n",
      "Train Epoch[1/2],step[508/600],tra_acc89.148622 %,loss:0.651542\n",
      "Train Epoch[1/2],step[510/600],tra_acc89.166667 %,loss:0.138091\n",
      "Train Epoch[1/2],step[512/600],tra_acc89.196777 %,loss:0.122743\n",
      "Train Epoch[1/2],step[514/600],tra_acc89.214494 %,loss:0.191869\n",
      "Train Epoch[1/2],step[516/600],tra_acc89.183624 %,loss:0.360739\n",
      "Train Epoch[1/2],step[518/600],tra_acc89.201255 %,loss:0.317472\n",
      "Train Epoch[1/2],step[520/600],tra_acc89.218750 %,loss:0.127023\n",
      "Train Epoch[1/2],step[522/600],tra_acc89.236111 %,loss:0.371223\n",
      "Train Epoch[1/2],step[524/600],tra_acc89.241412 %,loss:0.301351\n",
      "Train Epoch[1/2],step[526/600],tra_acc89.246673 %,loss:0.148440\n",
      "Train Epoch[1/2],step[528/600],tra_acc89.240057 %,loss:0.204783\n",
      "Train Epoch[1/2],step[530/600],tra_acc89.245283 %,loss:0.431729\n",
      "Train Epoch[1/2],step[532/600],tra_acc89.273966 %,loss:0.082707\n",
      "Train Epoch[1/2],step[534/600],tra_acc89.255618 %,loss:0.366601\n",
      "Train Epoch[1/2],step[536/600],tra_acc89.260728 %,loss:0.306033\n",
      "Train Epoch[1/2],step[538/600],tra_acc89.265799 %,loss:0.341504\n",
      "Train Epoch[1/2],step[540/600],tra_acc89.282407 %,loss:0.172289\n",
      "Train Epoch[1/2],step[542/600],tra_acc89.310424 %,loss:0.090065\n",
      "Train Epoch[1/2],step[544/600],tra_acc89.315257 %,loss:0.172032\n",
      "Train Epoch[1/2],step[546/600],tra_acc89.308608 %,loss:0.214578\n",
      "Train Epoch[1/2],step[548/600],tra_acc89.313412 %,loss:0.220036\n",
      "Train Epoch[1/2],step[550/600],tra_acc89.340909 %,loss:0.076465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [03:59,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[1/2],step[550/600],tra_acc:89.340909 %,bestAcc92.166667%,dev_acc92.083333 %,loss:0.076465\n",
      "Train Epoch[1/2],step[552/600],tra_acc89.311594 %,loss:0.241284\n",
      "Train Epoch[1/2],step[554/600],tra_acc89.327617 %,loss:0.396355\n",
      "Train Epoch[1/2],step[556/600],tra_acc89.366007 %,loss:0.105739\n",
      "Train Epoch[1/2],step[558/600],tra_acc89.370520 %,loss:0.117944\n",
      "Train Epoch[1/2],step[560/600],tra_acc89.352679 %,loss:0.313359\n",
      "Train Epoch[1/2],step[562/600],tra_acc89.368327 %,loss:0.182034\n",
      "Train Epoch[1/2],step[564/600],tra_acc89.383865 %,loss:0.132844\n",
      "Train Epoch[1/2],step[566/600],tra_acc89.333039 %,loss:0.374160\n",
      "Train Epoch[1/2],step[568/600],tra_acc89.337588 %,loss:0.132997\n",
      "Train Epoch[1/2],step[570/600],tra_acc89.364035 %,loss:0.065078\n",
      "Train Epoch[1/2],step[572/600],tra_acc89.368444 %,loss:0.218899\n",
      "Train Epoch[1/2],step[574/600],tra_acc89.405488 %,loss:0.100511\n",
      "Train Epoch[1/2],step[576/600],tra_acc89.409722 %,loss:0.082217\n",
      "Train Epoch[1/2],step[578/600],tra_acc89.413927 %,loss:0.105598\n",
      "Train Epoch[1/2],step[580/600],tra_acc89.439655 %,loss:0.079817\n",
      "Train Epoch[1/2],step[582/600],tra_acc89.465206 %,loss:0.098501\n",
      "Train Epoch[1/2],step[584/600],tra_acc89.469178 %,loss:0.253042\n",
      "Train Epoch[1/2],step[586/600],tra_acc89.473123 %,loss:0.175661\n",
      "Train Epoch[1/2],step[588/600],tra_acc89.498299 %,loss:0.220581\n",
      "Train Epoch[1/2],step[590/600],tra_acc89.512712 %,loss:0.062035\n",
      "Train Epoch[1/2],step[592/600],tra_acc89.516470 %,loss:0.125361\n",
      "Train Epoch[1/2],step[594/600],tra_acc89.541246 %,loss:0.038508\n",
      "Train Epoch[1/2],step[596/600],tra_acc89.565856 %,loss:0.075120\n",
      "Train Epoch[1/2],step[598/600],tra_acc89.569398 %,loss:0.135986\n",
      "Train Epoch[1/2],step[600/600],tra_acc89.593750 %,loss:0.316182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [03:58,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[1/2],step[600/600],tra_acc:89.593750 %,bestAcc92.916667%,dev_acc92.916667 %,loss:0.316182\n",
      "Train Epoch[2/2],step[2/600],tra_acc89.607558 %,loss:0.112155\n",
      "Train Epoch[2/2],step[4/600],tra_acc89.631623 %,loss:0.042632\n",
      "Train Epoch[2/2],step[6/600],tra_acc89.655528 %,loss:0.110118\n",
      "Train Epoch[2/2],step[8/600],tra_acc89.658717 %,loss:0.136336\n",
      "Train Epoch[2/2],step[10/600],tra_acc89.661885 %,loss:0.049295\n",
      "Train Epoch[2/2],step[12/600],tra_acc89.675245 %,loss:0.332547\n",
      "Train Epoch[2/2],step[14/600],tra_acc89.708876 %,loss:0.030710\n",
      "Train Epoch[2/2],step[16/600],tra_acc89.742289 %,loss:0.067195\n",
      "Train Epoch[2/2],step[18/600],tra_acc89.775485 %,loss:0.019404\n",
      "Train Epoch[2/2],step[20/600],tra_acc89.808468 %,loss:0.026674\n",
      "Train Epoch[2/2],step[22/600],tra_acc89.841238 %,loss:0.015758\n",
      "Train Epoch[2/2],step[24/600],tra_acc89.863782 %,loss:0.110879\n",
      "Train Epoch[2/2],step[26/600],tra_acc89.896166 %,loss:0.028290\n",
      "Train Epoch[2/2],step[28/600],tra_acc89.918392 %,loss:0.096047\n",
      "Train Epoch[2/2],step[30/600],tra_acc89.920635 %,loss:0.161658\n",
      "Train Epoch[2/2],step[32/600],tra_acc89.952532 %,loss:0.067774\n",
      "Train Epoch[2/2],step[34/600],tra_acc89.974369 %,loss:0.311526\n",
      "Train Epoch[2/2],step[36/600],tra_acc89.986242 %,loss:0.393955\n",
      "Train Epoch[2/2],step[38/600],tra_acc89.998041 %,loss:0.096727\n",
      "Train Epoch[2/2],step[40/600],tra_acc90.019531 %,loss:0.248130\n",
      "Train Epoch[2/2],step[42/600],tra_acc90.040888 %,loss:0.051779\n",
      "Train Epoch[2/2],step[44/600],tra_acc90.042702 %,loss:0.106107\n",
      "Train Epoch[2/2],step[46/600],tra_acc90.073529 %,loss:0.026059\n",
      "Train Epoch[2/2],step[48/600],tra_acc90.084877 %,loss:0.038404\n",
      "Train Epoch[2/2],step[50/600],tra_acc90.086538 %,loss:0.095459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [03:58,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[2/2],step[50/600],tra_acc:90.086538 %,bestAcc93.166667%,dev_acc93.166667 %,loss:0.095459\n",
      "Train Epoch[2/2],step[52/600],tra_acc90.097776 %,loss:0.176908\n",
      "Train Epoch[2/2],step[54/600],tra_acc90.118502 %,loss:0.122987\n",
      "Train Epoch[2/2],step[56/600],tra_acc90.139101 %,loss:0.103460\n",
      "Train Epoch[2/2],step[58/600],tra_acc90.169073 %,loss:0.068835\n",
      "Train Epoch[2/2],step[60/600],tra_acc90.189394 %,loss:0.098000\n",
      "Train Epoch[2/2],step[62/600],tra_acc90.209592 %,loss:0.079552\n",
      "Train Epoch[2/2],step[64/600],tra_acc90.220256 %,loss:0.178233\n",
      "Train Epoch[2/2],step[66/600],tra_acc90.212087 %,loss:0.608726\n",
      "Train Epoch[2/2],step[68/600],tra_acc90.232036 %,loss:0.264960\n",
      "Train Epoch[2/2],step[70/600],tra_acc90.242537 %,loss:0.184145\n",
      "Train Epoch[2/2],step[72/600],tra_acc90.252976 %,loss:0.181341\n",
      "Train Epoch[2/2],step[74/600],tra_acc90.254080 %,loss:0.255055\n",
      "Train Epoch[2/2],step[76/600],tra_acc90.282914 %,loss:0.032299\n",
      "Train Epoch[2/2],step[78/600],tra_acc90.283923 %,loss:0.065126\n",
      "Train Epoch[2/2],step[80/600],tra_acc90.312500 %,loss:0.055778\n",
      "Train Epoch[2/2],step[82/600],tra_acc90.313416 %,loss:0.315044\n",
      "Train Epoch[2/2],step[84/600],tra_acc90.314327 %,loss:0.144995\n",
      "Train Epoch[2/2],step[86/600],tra_acc90.333455 %,loss:0.076509\n",
      "Train Epoch[2/2],step[88/600],tra_acc90.352471 %,loss:0.088604\n",
      "Train Epoch[2/2],step[90/600],tra_acc90.380435 %,loss:0.059418\n",
      "Train Epoch[2/2],step[92/600],tra_acc90.381142 %,loss:0.068929\n",
      "Train Epoch[2/2],step[94/600],tra_acc90.399856 %,loss:0.035643\n",
      "Train Epoch[2/2],step[96/600],tra_acc90.427443 %,loss:0.047274\n",
      "Train Epoch[2/2],step[98/600],tra_acc90.454871 %,loss:0.059186\n",
      "Train Epoch[2/2],step[100/600],tra_acc90.464286 %,loss:0.166085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [03:57,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[2/2],step[100/600],tra_acc:90.464286 %,bestAcc93.166667%,dev_acc91.500000 %,loss:0.166085\n",
      "Train Epoch[2/2],step[102/600],tra_acc90.491453 %,loss:0.079678\n",
      "Train Epoch[2/2],step[104/600],tra_acc90.518466 %,loss:0.027104\n",
      "Train Epoch[2/2],step[106/600],tra_acc90.527620 %,loss:0.114103\n",
      "Train Epoch[2/2],step[108/600],tra_acc90.554379 %,loss:0.010106\n",
      "Train Epoch[2/2],step[110/600],tra_acc90.580986 %,loss:0.005324\n",
      "Train Epoch[2/2],step[112/600],tra_acc90.598666 %,loss:0.249382\n",
      "Train Epoch[2/2],step[114/600],tra_acc90.598739 %,loss:0.094185\n",
      "Train Epoch[2/2],step[116/600],tra_acc90.590084 %,loss:0.009122\n",
      "Train Epoch[2/2],step[118/600],tra_acc90.607591 %,loss:0.060076\n",
      "Train Epoch[2/2],step[120/600],tra_acc90.607639 %,loss:0.381183\n",
      "Train Epoch[2/2],step[122/600],tra_acc90.599030 %,loss:0.096547\n",
      "Train Epoch[2/2],step[124/600],tra_acc90.625000 %,loss:0.086935\n",
      "Train Epoch[2/2],step[126/600],tra_acc90.625000 %,loss:0.280258\n",
      "Train Epoch[2/2],step[128/600],tra_acc90.650755 %,loss:0.037220\n",
      "Train Epoch[2/2],step[130/600],tra_acc90.676370 %,loss:0.021458\n",
      "Train Epoch[2/2],step[132/600],tra_acc90.701844 %,loss:0.032339\n",
      "Train Epoch[2/2],step[134/600],tra_acc90.718665 %,loss:0.050284\n",
      "Train Epoch[2/2],step[136/600],tra_acc90.743886 %,loss:0.036941\n",
      "Train Epoch[2/2],step[138/600],tra_acc90.768970 %,loss:0.090572\n",
      "Train Epoch[2/2],step[140/600],tra_acc90.785473 %,loss:0.092243\n",
      "Train Epoch[2/2],step[142/600],tra_acc90.785040 %,loss:0.035029\n",
      "Train Epoch[2/2],step[144/600],tra_acc90.784610 %,loss:0.172157\n",
      "Train Epoch[2/2],step[146/600],tra_acc90.767426 %,loss:0.510830\n",
      "Train Epoch[2/2],step[148/600],tra_acc90.783757 %,loss:0.104256\n",
      "Train Epoch[2/2],step[150/600],tra_acc90.800000 %,loss:0.084780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [03:58,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[2/2],step[150/600],tra_acc:90.800000 %,bestAcc93.166667%,dev_acc92.916667 %,loss:0.084780\n",
      "Train Epoch[2/2],step[152/600],tra_acc90.791223 %,loss:0.215840\n",
      "Train Epoch[2/2],step[154/600],tra_acc90.807361 %,loss:0.051071\n",
      "Train Epoch[2/2],step[156/600],tra_acc90.806878 %,loss:0.109129\n",
      "Train Epoch[2/2],step[158/600],tra_acc90.822889 %,loss:0.021396\n",
      "Train Epoch[2/2],step[160/600],tra_acc90.838816 %,loss:0.055829\n",
      "Train Epoch[2/2],step[162/600],tra_acc90.846457 %,loss:0.192246\n",
      "Train Epoch[2/2],step[164/600],tra_acc90.870419 %,loss:0.026778\n",
      "Train Epoch[2/2],step[166/600],tra_acc90.894256 %,loss:0.072874\n",
      "Train Epoch[2/2],step[168/600],tra_acc90.901693 %,loss:0.119240\n",
      "Train Epoch[2/2],step[170/600],tra_acc90.909091 %,loss:0.334063\n",
      "Train Epoch[2/2],step[172/600],tra_acc90.908355 %,loss:0.107063\n",
      "Train Epoch[2/2],step[174/600],tra_acc90.931848 %,loss:0.050518\n",
      "Train Epoch[2/2],step[176/600],tra_acc90.939111 %,loss:0.117987\n",
      "Train Epoch[2/2],step[178/600],tra_acc90.938303 %,loss:0.083407\n",
      "Train Epoch[2/2],step[180/600],tra_acc90.961538 %,loss:0.067120\n",
      "Train Epoch[2/2],step[182/600],tra_acc90.976662 %,loss:0.034590\n",
      "Train Epoch[2/2],step[184/600],tra_acc90.983737 %,loss:0.238727\n",
      "Train Epoch[2/2],step[186/600],tra_acc90.966921 %,loss:0.079589\n",
      "Train Epoch[2/2],step[188/600],tra_acc90.981916 %,loss:0.190577\n",
      "Train Epoch[2/2],step[190/600],tra_acc90.996835 %,loss:0.072003\n",
      "Train Epoch[2/2],step[192/600],tra_acc91.019571 %,loss:0.028679\n",
      "Train Epoch[2/2],step[194/600],tra_acc91.042191 %,loss:0.012259\n",
      "Train Epoch[2/2],step[196/600],tra_acc91.064698 %,loss:0.020210\n",
      "Train Epoch[2/2],step[198/600],tra_acc91.079261 %,loss:0.060544\n",
      "Train Epoch[2/2],step[200/600],tra_acc91.093750 %,loss:0.255332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [03:59,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[2/2],step[200/600],tra_acc:91.093750 %,bestAcc93.166667%,dev_acc92.666667 %,loss:0.255332\n",
      "Train Epoch[2/2],step[202/600],tra_acc91.100374 %,loss:0.079110\n",
      "Train Epoch[2/2],step[204/600],tra_acc91.106965 %,loss:0.073896\n",
      "Train Epoch[2/2],step[206/600],tra_acc91.121278 %,loss:0.019093\n",
      "Train Epoch[2/2],step[208/600],tra_acc91.135520 %,loss:0.249191\n",
      "Train Epoch[2/2],step[210/600],tra_acc91.134259 %,loss:0.414974\n",
      "Train Epoch[2/2],step[212/600],tra_acc91.156096 %,loss:0.019180\n",
      "Train Epoch[2/2],step[214/600],tra_acc91.177826 %,loss:0.010200\n",
      "Train Epoch[2/2],step[216/600],tra_acc91.199449 %,loss:0.011773\n",
      "Train Epoch[2/2],step[218/600],tra_acc91.213325 %,loss:0.046600\n",
      "Train Epoch[2/2],step[220/600],tra_acc91.219512 %,loss:0.211401\n",
      "Train Epoch[2/2],step[222/600],tra_acc91.240876 %,loss:0.036961\n",
      "Train Epoch[2/2],step[224/600],tra_acc91.254551 %,loss:0.228608\n",
      "Train Epoch[2/2],step[226/600],tra_acc91.275726 %,loss:0.016616\n",
      "Train Epoch[2/2],step[228/600],tra_acc91.289251 %,loss:0.008761\n",
      "Train Epoch[2/2],step[230/600],tra_acc91.302711 %,loss:0.089567\n",
      "Train Epoch[2/2],step[232/600],tra_acc91.323618 %,loss:0.014464\n",
      "Train Epoch[2/2],step[234/600],tra_acc91.321942 %,loss:0.084583\n",
      "Train Epoch[2/2],step[236/600],tra_acc91.342703 %,loss:0.012139\n",
      "Train Epoch[2/2],step[238/600],tra_acc91.348449 %,loss:0.066019\n",
      "Train Epoch[2/2],step[240/600],tra_acc91.369048 %,loss:0.013792\n",
      "Train Epoch[2/2],step[242/600],tra_acc91.389549 %,loss:0.011716\n",
      "Train Epoch[2/2],step[244/600],tra_acc91.395142 %,loss:0.129663\n",
      "Train Epoch[2/2],step[246/600],tra_acc91.408097 %,loss:0.066855\n",
      "Train Epoch[2/2],step[248/600],tra_acc91.428361 %,loss:0.019867\n",
      "Train Epoch[2/2],step[250/600],tra_acc91.426471 %,loss:0.305708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [03:58,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[2/2],step[250/600],tra_acc:91.426471 %,bestAcc93.583333%,dev_acc93.583333 %,loss:0.305708\n",
      "Train Epoch[2/2],step[252/600],tra_acc91.439261 %,loss:0.028556\n",
      "Train Epoch[2/2],step[254/600],tra_acc91.422717 %,loss:0.282940\n",
      "Train Epoch[2/2],step[256/600],tra_acc91.442757 %,loss:0.060341\n",
      "Train Epoch[2/2],step[258/600],tra_acc91.448135 %,loss:0.031676\n",
      "Train Epoch[2/2],step[260/600],tra_acc91.460756 %,loss:0.016549\n",
      "Train Epoch[2/2],step[262/600],tra_acc91.480568 %,loss:0.052185\n",
      "Train Epoch[2/2],step[264/600],tra_acc91.478588 %,loss:0.087842\n",
      "Train Epoch[2/2],step[266/600],tra_acc91.483834 %,loss:0.285417\n",
      "Train Epoch[2/2],step[268/600],tra_acc91.489055 %,loss:0.120830\n",
      "Train Epoch[2/2],step[270/600],tra_acc91.501437 %,loss:0.168090\n",
      "Train Epoch[2/2],step[272/600],tra_acc91.520929 %,loss:0.044661\n",
      "Train Epoch[2/2],step[274/600],tra_acc91.518879 %,loss:0.199233\n",
      "Train Epoch[2/2],step[276/600],tra_acc91.538242 %,loss:0.037744\n",
      "Train Epoch[2/2],step[278/600],tra_acc91.557517 %,loss:0.127501\n",
      "Train Epoch[2/2],step[280/600],tra_acc91.534091 %,loss:0.202612\n",
      "Train Epoch[2/2],step[282/600],tra_acc91.546202 %,loss:0.064942\n",
      "Train Epoch[2/2],step[284/600],tra_acc91.537048 %,loss:0.256892\n",
      "Train Epoch[2/2],step[286/600],tra_acc91.549097 %,loss:0.025540\n",
      "Train Epoch[2/2],step[288/600],tra_acc91.561092 %,loss:0.004983\n",
      "Train Epoch[2/2],step[290/600],tra_acc91.580056 %,loss:0.053747\n",
      "Train Epoch[2/2],step[292/600],tra_acc91.584922 %,loss:0.105714\n",
      "Train Epoch[2/2],step[294/600],tra_acc91.589765 %,loss:0.227085\n",
      "Train Epoch[2/2],step[296/600],tra_acc91.594587 %,loss:0.013893\n",
      "Train Epoch[2/2],step[298/600],tra_acc91.599388 %,loss:0.202692\n",
      "Train Epoch[2/2],step[300/600],tra_acc91.618056 %,loss:0.060921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [03:58,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[2/2],step[300/600],tra_acc:91.618056 %,bestAcc93.583333%,dev_acc92.583333 %,loss:0.060921\n",
      "Train Epoch[2/2],step[302/600],tra_acc91.629712 %,loss:0.046311\n",
      "Train Epoch[2/2],step[304/600],tra_acc91.648230 %,loss:0.027955\n",
      "Train Epoch[2/2],step[306/600],tra_acc91.659768 %,loss:0.070474\n",
      "Train Epoch[2/2],step[308/600],tra_acc91.678139 %,loss:0.013178\n",
      "Train Epoch[2/2],step[310/600],tra_acc91.689560 %,loss:0.043633\n",
      "Train Epoch[2/2],step[312/600],tra_acc91.694079 %,loss:0.169116\n",
      "Train Epoch[2/2],step[314/600],tra_acc91.705416 %,loss:0.030090\n",
      "Train Epoch[2/2],step[316/600],tra_acc91.716703 %,loss:0.034378\n",
      "Train Epoch[2/2],step[318/600],tra_acc91.727941 %,loss:0.298305\n",
      "Train Epoch[2/2],step[320/600],tra_acc91.732337 %,loss:0.009025\n",
      "Train Epoch[2/2],step[322/600],tra_acc91.750271 %,loss:0.013562\n",
      "Train Epoch[2/2],step[324/600],tra_acc91.761364 %,loss:0.316607\n",
      "Train Epoch[2/2],step[326/600],tra_acc91.765659 %,loss:0.124447\n",
      "Train Epoch[2/2],step[328/600],tra_acc91.783405 %,loss:0.012186\n",
      "Train Epoch[2/2],step[330/600],tra_acc91.801075 %,loss:0.019884\n",
      "Train Epoch[2/2],step[332/600],tra_acc91.805258 %,loss:0.175654\n",
      "Train Epoch[2/2],step[334/600],tra_acc91.822805 %,loss:0.008026\n",
      "Train Epoch[2/2],step[336/600],tra_acc91.840278 %,loss:0.010406\n",
      "Train Epoch[2/2],step[338/600],tra_acc91.844350 %,loss:0.083508\n",
      "Train Epoch[2/2],step[340/600],tra_acc91.855053 %,loss:0.218442\n",
      "Train Epoch[2/2],step[342/600],tra_acc91.865711 %,loss:0.109527\n",
      "Train Epoch[2/2],step[344/600],tra_acc91.876324 %,loss:0.123941\n",
      "Train Epoch[2/2],step[346/600],tra_acc91.886892 %,loss:0.080310\n",
      "Train Epoch[2/2],step[348/600],tra_acc91.897416 %,loss:0.019499\n",
      "Train Epoch[2/2],step[350/600],tra_acc91.914474 %,loss:0.015222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [03:58,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[2/2],step[350/600],tra_acc:91.914474 %,bestAcc93.583333%,dev_acc92.583333 %,loss:0.015222\n",
      "Train Epoch[2/2],step[352/600],tra_acc91.918330 %,loss:0.168031\n",
      "Train Epoch[2/2],step[354/600],tra_acc91.928721 %,loss:0.031676\n",
      "Train Epoch[2/2],step[356/600],tra_acc91.945607 %,loss:0.015913\n",
      "Train Epoch[2/2],step[358/600],tra_acc91.962422 %,loss:0.018794\n",
      "Train Epoch[2/2],step[360/600],tra_acc91.966146 %,loss:0.147029\n",
      "Train Epoch[2/2],step[362/600],tra_acc91.976351 %,loss:0.060557\n",
      "Train Epoch[2/2],step[364/600],tra_acc91.992998 %,loss:0.043504\n",
      "Train Epoch[2/2],step[366/600],tra_acc91.996636 %,loss:0.081367\n",
      "Train Epoch[2/2],step[368/600],tra_acc92.006715 %,loss:0.024791\n",
      "Train Epoch[2/2],step[370/600],tra_acc92.016753 %,loss:0.089874\n",
      "Train Epoch[2/2],step[372/600],tra_acc92.033179 %,loss:0.038698\n",
      "Train Epoch[2/2],step[374/600],tra_acc92.030287 %,loss:0.003802\n",
      "Train Epoch[2/2],step[376/600],tra_acc92.046619 %,loss:0.029959\n",
      "Train Epoch[2/2],step[378/600],tra_acc92.062883 %,loss:0.005137\n",
      "Train Epoch[2/2],step[380/600],tra_acc92.072704 %,loss:0.006846\n",
      "Train Epoch[2/2],step[382/600],tra_acc92.088849 %,loss:0.052578\n",
      "Train Epoch[2/2],step[384/600],tra_acc92.085874 %,loss:0.234075\n",
      "Train Epoch[2/2],step[386/600],tra_acc92.095588 %,loss:0.043347\n",
      "Train Epoch[2/2],step[388/600],tra_acc92.111589 %,loss:0.033743\n",
      "Train Epoch[2/2],step[390/600],tra_acc92.114899 %,loss:0.360154\n",
      "Train Epoch[2/2],step[392/600],tra_acc92.111895 %,loss:0.561249\n",
      "Train Epoch[2/2],step[394/600],tra_acc92.127767 %,loss:0.093896\n",
      "Train Epoch[2/2],step[396/600],tra_acc92.137299 %,loss:0.093419\n",
      "Train Epoch[2/2],step[398/600],tra_acc92.146794 %,loss:0.027311\n",
      "Train Epoch[2/2],step[400/600],tra_acc92.150000 %,loss:0.021796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [03:58,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[2/2],step[400/600],tra_acc:92.150000 %,bestAcc93.583333%,dev_acc93.416667 %,loss:0.021796\n",
      "Train Epoch[2/2],step[402/600],tra_acc92.165669 %,loss:0.051693\n",
      "Train Epoch[2/2],step[404/600],tra_acc92.181275 %,loss:0.022649\n",
      "Train Epoch[2/2],step[406/600],tra_acc92.184394 %,loss:0.217616\n",
      "Train Epoch[2/2],step[408/600],tra_acc92.199901 %,loss:0.065676\n",
      "Train Epoch[2/2],step[410/600],tra_acc92.196782 %,loss:0.210993\n",
      "Train Epoch[2/2],step[412/600],tra_acc92.199852 %,loss:0.126941\n",
      "Train Epoch[2/2],step[414/600],tra_acc92.215237 %,loss:0.022777\n",
      "Train Epoch[2/2],step[416/600],tra_acc92.230561 %,loss:0.040117\n",
      "Train Epoch[2/2],step[418/600],tra_acc92.245825 %,loss:0.026756\n",
      "Train Epoch[2/2],step[420/600],tra_acc92.242647 %,loss:0.145742\n",
      "Train Epoch[2/2],step[422/600],tra_acc92.245597 %,loss:0.096700\n",
      "Train Epoch[2/2],step[424/600],tra_acc92.248535 %,loss:0.396861\n",
      "Train Epoch[2/2],step[426/600],tra_acc92.257554 %,loss:0.151851\n",
      "Train Epoch[2/2],step[428/600],tra_acc92.266537 %,loss:0.006214\n",
      "Train Epoch[2/2],step[430/600],tra_acc92.251214 %,loss:0.301943\n",
      "Train Epoch[2/2],step[432/600],tra_acc92.266231 %,loss:0.043052\n",
      "Train Epoch[2/2],step[434/600],tra_acc92.269101 %,loss:0.140809\n",
      "Train Epoch[2/2],step[436/600],tra_acc92.271959 %,loss:0.234712\n",
      "Train Epoch[2/2],step[438/600],tra_acc92.280829 %,loss:0.343557\n",
      "Train Epoch[2/2],step[440/600],tra_acc92.289663 %,loss:0.068498\n",
      "Train Epoch[2/2],step[442/600],tra_acc92.286468 %,loss:0.214776\n",
      "Train Epoch[2/2],step[444/600],tra_acc92.289272 %,loss:0.139067\n",
      "Train Epoch[2/2],step[446/600],tra_acc92.286090 %,loss:0.264809\n",
      "Train Epoch[2/2],step[448/600],tra_acc92.294847 %,loss:0.029105\n",
      "Train Epoch[2/2],step[450/600],tra_acc92.309524 %,loss:0.043823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [03:59,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[2/2],step[450/600],tra_acc:92.309524 %,bestAcc93.666667%,dev_acc93.666667 %,loss:0.043823\n",
      "Train Epoch[2/2],step[452/600],tra_acc92.324144 %,loss:0.043059\n",
      "Train Epoch[2/2],step[454/600],tra_acc92.314991 %,loss:0.214980\n",
      "Train Epoch[2/2],step[456/600],tra_acc92.323627 %,loss:0.089032\n",
      "Train Epoch[2/2],step[458/600],tra_acc92.332231 %,loss:0.026956\n",
      "Train Epoch[2/2],step[460/600],tra_acc92.340802 %,loss:0.071582\n",
      "Train Epoch[2/2],step[462/600],tra_acc92.331685 %,loss:0.400912\n",
      "Train Epoch[2/2],step[464/600],tra_acc92.346100 %,loss:0.012798\n",
      "Train Epoch[2/2],step[466/600],tra_acc92.360460 %,loss:0.065987\n",
      "Train Epoch[2/2],step[468/600],tra_acc92.368914 %,loss:0.095042\n",
      "Train Epoch[2/2],step[470/600],tra_acc92.377336 %,loss:0.035781\n",
      "Train Epoch[2/2],step[472/600],tra_acc92.374067 %,loss:0.798726\n",
      "Train Epoch[2/2],step[474/600],tra_acc92.382449 %,loss:0.311793\n",
      "Train Epoch[2/2],step[476/600],tra_acc92.379182 %,loss:0.500892\n",
      "Train Epoch[2/2],step[478/600],tra_acc92.393321 %,loss:0.042323\n",
      "Train Epoch[2/2],step[480/600],tra_acc92.407407 %,loss:0.061243\n",
      "Train Epoch[2/2],step[482/600],tra_acc92.415665 %,loss:0.093173\n",
      "Train Epoch[2/2],step[484/600],tra_acc92.423893 %,loss:0.139532\n",
      "Train Epoch[2/2],step[486/600],tra_acc92.437845 %,loss:0.110533\n",
      "Train Epoch[2/2],step[488/600],tra_acc92.451746 %,loss:0.032156\n",
      "Train Epoch[2/2],step[490/600],tra_acc92.459862 %,loss:0.074346\n",
      "Train Epoch[2/2],step[492/600],tra_acc92.467949 %,loss:0.016790\n",
      "Train Epoch[2/2],step[494/600],tra_acc92.476005 %,loss:0.184115\n",
      "Train Epoch[2/2],step[496/600],tra_acc92.478330 %,loss:0.351001\n",
      "Train Epoch[2/2],step[498/600],tra_acc92.492031 %,loss:0.058267\n",
      "Train Epoch[2/2],step[500/600],tra_acc92.500000 %,loss:0.228871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [03:59,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[2/2],step[500/600],tra_acc:92.500000 %,bestAcc93.666667%,dev_acc93.166667 %,loss:0.228871\n",
      "Train Epoch[2/2],step[502/600],tra_acc92.490926 %,loss:0.240640\n",
      "Train Epoch[2/2],step[504/600],tra_acc92.498868 %,loss:0.166946\n",
      "Train Epoch[2/2],step[506/600],tra_acc92.506781 %,loss:0.099649\n",
      "Train Epoch[2/2],step[508/600],tra_acc92.503384 %,loss:0.155181\n",
      "Train Epoch[2/2],step[510/600],tra_acc92.500000 %,loss:0.167626\n",
      "Train Epoch[2/2],step[512/600],tra_acc92.507869 %,loss:0.070509\n",
      "Train Epoch[2/2],step[514/600],tra_acc92.515709 %,loss:0.027458\n",
      "Train Epoch[2/2],step[516/600],tra_acc92.517921 %,loss:0.165443\n",
      "Train Epoch[2/2],step[518/600],tra_acc92.520125 %,loss:0.124751\n",
      "Train Epoch[2/2],step[520/600],tra_acc92.511161 %,loss:0.094959\n",
      "Train Epoch[2/2],step[522/600],tra_acc92.524510 %,loss:0.075091\n",
      "Train Epoch[2/2],step[524/600],tra_acc92.521130 %,loss:0.281581\n",
      "Train Epoch[2/2],step[526/600],tra_acc92.523313 %,loss:0.085952\n",
      "Train Epoch[2/2],step[528/600],tra_acc92.525488 %,loss:0.034318\n",
      "Train Epoch[2/2],step[530/600],tra_acc92.538717 %,loss:0.044891\n",
      "Train Epoch[2/2],step[532/600],tra_acc92.535336 %,loss:0.106376\n",
      "Train Epoch[2/2],step[534/600],tra_acc92.537478 %,loss:0.158579\n",
      "Train Epoch[2/2],step[536/600],tra_acc92.545114 %,loss:0.176503\n",
      "Train Epoch[2/2],step[538/600],tra_acc92.558216 %,loss:0.074543\n",
      "Train Epoch[2/2],step[540/600],tra_acc92.565789 %,loss:0.127323\n",
      "Train Epoch[2/2],step[542/600],tra_acc92.573336 %,loss:0.068989\n",
      "Train Epoch[2/2],step[544/600],tra_acc92.575393 %,loss:0.007229\n",
      "Train Epoch[2/2],step[546/600],tra_acc92.582897 %,loss:0.010383\n",
      "Train Epoch[2/2],step[548/600],tra_acc92.595819 %,loss:0.055567\n",
      "Train Epoch[2/2],step[550/600],tra_acc92.608696 %,loss:0.036323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [03:59,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[2/2],step[550/600],tra_acc:92.608696 %,bestAcc93.666667%,dev_acc93.250000 %,loss:0.036323\n",
      "Train Epoch[2/2],step[552/600],tra_acc92.621528 %,loss:0.020289\n",
      "Train Epoch[2/2],step[554/600],tra_acc92.634315 %,loss:0.007628\n",
      "Train Epoch[2/2],step[556/600],tra_acc92.641652 %,loss:0.060804\n",
      "Train Epoch[2/2],step[558/600],tra_acc92.654361 %,loss:0.005590\n",
      "Train Epoch[2/2],step[560/600],tra_acc92.650862 %,loss:0.197503\n",
      "Train Epoch[2/2],step[562/600],tra_acc92.663511 %,loss:0.006199\n",
      "Train Epoch[2/2],step[564/600],tra_acc92.670747 %,loss:0.142027\n",
      "Train Epoch[2/2],step[566/600],tra_acc92.672599 %,loss:0.217294\n",
      "Train Epoch[2/2],step[568/600],tra_acc92.685146 %,loss:0.006395\n",
      "Train Epoch[2/2],step[570/600],tra_acc92.676282 %,loss:0.064290\n",
      "Train Epoch[2/2],step[572/600],tra_acc92.688780 %,loss:0.009828\n",
      "Train Epoch[2/2],step[574/600],tra_acc92.701235 %,loss:0.018620\n",
      "Train Epoch[2/2],step[576/600],tra_acc92.703019 %,loss:0.060691\n",
      "Train Epoch[2/2],step[578/600],tra_acc92.710102 %,loss:0.397717\n",
      "Train Epoch[2/2],step[580/600],tra_acc92.711864 %,loss:0.115570\n",
      "Train Epoch[2/2],step[582/600],tra_acc92.703046 %,loss:0.230354\n",
      "Train Epoch[2/2],step[584/600],tra_acc92.715372 %,loss:0.083031\n",
      "Train Epoch[2/2],step[586/600],tra_acc92.727656 %,loss:0.032168\n",
      "Train Epoch[2/2],step[588/600],tra_acc92.729377 %,loss:0.119615\n",
      "Train Epoch[2/2],step[590/600],tra_acc92.741597 %,loss:0.037852\n",
      "Train Epoch[2/2],step[592/600],tra_acc92.753775 %,loss:0.020749\n",
      "Train Epoch[2/2],step[594/600],tra_acc92.760678 %,loss:0.036106\n",
      "Train Epoch[2/2],step[596/600],tra_acc92.767559 %,loss:0.065649\n",
      "Train Epoch[2/2],step[598/600],tra_acc92.779633 %,loss:0.065189\n",
      "Train Epoch[2/2],step[600/600],tra_acc92.781250 %,loss:0.061845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev Itreation:: 75it [04:01,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV Epoch[2/2],step[600/600],tra_acc:92.781250 %,bestAcc94.166667%,dev_acc94.166667 %,loss:0.061845\n"
     ]
    }
   ],
   "source": [
    "#调用训练函数进行训练与验证\n",
    "train(model,train_loader,validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaa37bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义预测函数\n",
    "import torch.nn.functional as F\n",
    "def predict(model,test_loader):\n",
    "    model.to(device)\n",
    "    # 将模型中的某些特定层或部分切换到评估模式\n",
    "    model.eval()\n",
    "    predicts = []\n",
    "    predict_probs = []\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for step, (input_ids,token_type_ids,attention_mask,labels) in enumerate(test_loader): \n",
    "            input_ids,token_type_ids,attention_mask,labels=input_ids.to(device),token_type_ids.to(device),attention_mask.to(device),labels.to(device)\n",
    "            out_put = model(input_ids,token_type_ids,attention_mask)\n",
    "            _, predict = torch.max(out_put.data, 1)\n",
    "            pre_numpy = predict.cpu().numpy().tolist()\n",
    "            print(\"pre_numpy:\")\n",
    "            print(pre_numpy)\n",
    "            print(\"labels:\")\n",
    "            print(labels)\n",
    "            predicts.extend(pre_numpy)\n",
    "            probs = F.softmax(out_put, dim=1).detach().cpu().numpy().tolist()\n",
    "            predict_probs.extend(probs)\n",
    "            correct += (predict==labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        res = correct / total\n",
    "        print('**************结果**************\\npredict_Accuracy : {} %'.format(100 * res))\n",
    "        #返回预测结果和预测的概率\n",
    "        return predicts,predict_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f7380a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction start !\n",
      "**************说明**************\n",
      "一、1代表正向情感，0代表负面情感。\n",
      "二、预测值代表“训练好的模型对该句子所预测的情感”，对应变量pre_numpy。\n",
      "三、真实值代表“实际上该句子所表达的情感”，对应变量labels。\n",
      "********************************\n",
      "pre_numpy:\n",
      "[0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels:\n",
      "tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1])\n",
      "pre_numpy:\n",
      "[0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0]\n",
      "labels:\n",
      "tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
      "labels:\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0])\n",
      "pre_numpy:\n",
      "[1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n",
      "labels:\n",
      "tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1])\n",
      "pre_numpy:\n",
      "[0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0]\n",
      "labels:\n",
      "tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0]\n",
      "labels:\n",
      "tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0])\n",
      "pre_numpy:\n",
      "[0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0]\n",
      "labels:\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0])\n",
      "pre_numpy:\n",
      "[1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0]\n",
      "labels:\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1])\n",
      "pre_numpy:\n",
      "[1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1]\n",
      "labels:\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "pre_numpy:\n",
      "[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "labels:\n",
      "tensor([0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1])\n",
      "pre_numpy:\n",
      "[1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0]\n",
      "labels:\n",
      "tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "labels:\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])\n",
      "pre_numpy:\n",
      "[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1]\n",
      "labels:\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1])\n",
      "pre_numpy:\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "labels:\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1])\n",
      "pre_numpy:\n",
      "[1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]\n",
      "labels:\n",
      "tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0])\n",
      "pre_numpy:\n",
      "[1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "labels:\n",
      "tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0])\n",
      "pre_numpy:\n",
      "[0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1]\n",
      "labels:\n",
      "tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1])\n",
      "pre_numpy:\n",
      "[1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1]\n",
      "labels:\n",
      "tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1])\n",
      "pre_numpy:\n",
      "[0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "labels:\n",
      "tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1]\n",
      "labels:\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1])\n",
      "pre_numpy:\n",
      "[1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0]\n",
      "labels:\n",
      "tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1])\n",
      "pre_numpy:\n",
      "[1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "labels:\n",
      "tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "pre_numpy:\n",
      "[1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1]\n",
      "labels:\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1])\n",
      "pre_numpy:\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1]\n",
      "labels:\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "pre_numpy:\n",
      "[1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1]\n",
      "labels:\n",
      "tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1])\n",
      "pre_numpy:\n",
      "[1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "labels:\n",
      "tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0])\n",
      "pre_numpy:\n",
      "[1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0]\n",
      "labels:\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0])\n",
      "pre_numpy:\n",
      "[0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0]\n",
      "labels:\n",
      "tensor([0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0])\n",
      "pre_numpy:\n",
      "[1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1]\n",
      "labels:\n",
      "tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1])\n",
      "pre_numpy:\n",
      "[1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1]\n",
      "labels:\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1])\n",
      "pre_numpy:\n",
      "[0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0]\n",
      "labels:\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1]\n",
      "labels:\n",
      "tensor([1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1])\n",
      "pre_numpy:\n",
      "[1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0]\n",
      "labels:\n",
      "tensor([1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1])\n",
      "pre_numpy:\n",
      "[0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0]\n",
      "labels:\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0])\n",
      "pre_numpy:\n",
      "[1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1]\n",
      "labels:\n",
      "tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1])\n",
      "pre_numpy:\n",
      "[0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0]\n",
      "labels:\n",
      "tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "pre_numpy:\n",
      "[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1]\n",
      "labels:\n",
      "tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1])\n",
      "pre_numpy:\n",
      "[1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "labels:\n",
      "tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0])\n",
      "pre_numpy:\n",
      "[1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0]\n",
      "labels:\n",
      "tensor([1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0])\n",
      "pre_numpy:\n",
      "[1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "labels:\n",
      "tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0]\n",
      "labels:\n",
      "tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0])\n",
      "pre_numpy:\n",
      "[1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0]\n",
      "labels:\n",
      "tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "labels:\n",
      "tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n",
      "pre_numpy:\n",
      "[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "labels:\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1])\n",
      "pre_numpy:\n",
      "[0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
      "labels:\n",
      "tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0])\n",
      "pre_numpy:\n",
      "[0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0]\n",
      "labels:\n",
      "tensor([0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1]\n",
      "labels:\n",
      "tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0])\n",
      "pre_numpy:\n",
      "[1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0]\n",
      "labels:\n",
      "tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0])\n",
      "pre_numpy:\n",
      "[0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "labels:\n",
      "tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "labels:\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0])\n",
      "pre_numpy:\n",
      "[1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1]\n",
      "labels:\n",
      "tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1])\n",
      "pre_numpy:\n",
      "[0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0]\n",
      "labels:\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "labels:\n",
      "tensor([1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0])\n",
      "pre_numpy:\n",
      "[1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0]\n",
      "labels:\n",
      "tensor([1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1]\n",
      "labels:\n",
      "tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "pre_numpy:\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n",
      "labels:\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1]\n",
      "labels:\n",
      "tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1])\n",
      "pre_numpy:\n",
      "[1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1]\n",
      "labels:\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1])\n",
      "pre_numpy:\n",
      "[1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "labels:\n",
      "tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1])\n",
      "pre_numpy:\n",
      "[0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0]\n",
      "labels:\n",
      "tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1])\n",
      "pre_numpy:\n",
      "[1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0]\n",
      "labels:\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0]\n",
      "labels:\n",
      "tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0])\n",
      "pre_numpy:\n",
      "[1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0]\n",
      "labels:\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0])\n",
      "pre_numpy:\n",
      "[1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
      "labels:\n",
      "tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "labels:\n",
      "tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1])\n",
      "pre_numpy:\n",
      "[1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n",
      "labels:\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0])\n",
      "pre_numpy:\n",
      "[1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0]\n",
      "labels:\n",
      "tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1]\n",
      "labels:\n",
      "tensor([0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1])\n",
      "pre_numpy:\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]\n",
      "labels:\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1])\n",
      "pre_numpy:\n",
      "[0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "labels:\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0])\n",
      "pre_numpy:\n",
      "[0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]\n",
      "labels:\n",
      "tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1])\n",
      "pre_numpy:\n",
      "[0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "labels:\n",
      "tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1])\n",
      "pre_numpy:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1]\n",
      "labels:\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1])\n",
      "pre_numpy:\n",
      "[0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "labels:\n",
      "tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n",
      "pre_numpy:\n",
      "[0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0]\n",
      "labels:\n",
      "tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0])\n",
      "**************结果**************\n",
      "predict_Accuracy : 89.16666666666667 %\n"
     ]
    }
   ],
   "source": [
    "# 定义预测函数\n",
    "import torch.nn.functional as F\n",
    "def predict(model,test_loader):\n",
    "    model.to(device)\n",
    "    # 将模型中的某些特定层或部分切换到评估模式\n",
    "    model.eval()\n",
    "    predicts = []\n",
    "    predict_probs = []\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for step, (input_ids,token_type_ids,attention_mask,labels) in enumerate(test_loader): \n",
    "            input_ids,token_type_ids,attention_mask,labels=input_ids.to(device),token_type_ids.to(device),attention_mask.to(device),labels.to(device)\n",
    "            out_put = model(input_ids,token_type_ids,attention_mask)\n",
    "            _, predict = torch.max(out_put.data, 1)\n",
    "            pre_numpy = predict.cpu().numpy().tolist()\n",
    "            print(\"pre_numpy:\")\n",
    "            print(pre_numpy)\n",
    "            print(\"labels:\")\n",
    "            print(labels)\n",
    "            predicts.extend(pre_numpy)\n",
    "            probs = F.softmax(out_put, dim=1).detach().cpu().numpy().tolist()\n",
    "            predict_probs.extend(probs)\n",
    "            correct += (predict==labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        res = correct / total\n",
    "        print('**************结果**************\\npredict_Accuracy : {} %'.format(100 * res))\n",
    "        #返回预测结果和预测的概率\n",
    "        return predicts,predict_probs\n",
    "\n",
    "# 使用训练好的模型进行预测\n",
    "# 1、加载测试数据集\n",
    "dataset_test = dataset['test']\n",
    "dataset_test_ts = load_data(dataset_test)\n",
    "test_loader = DataLoader(dataset=dataset_test_ts, batch_size=batch_size, shuffle=False) \n",
    "# 2、加载训练好的模型\n",
    "path = r'E:\\output\\savedmodel\\model_new.pkl'\n",
    "Trained_model = torch.load(path)\n",
    "# 3、开始预测\n",
    "print(\"The prediction start !\\n**************说明**************\\n一、1代表正向情感，0代表负面情感。\\n二、预测值代表“训练好的模型对该句子所预测的情感”，对应变量pre_numpy。\\n三、真实值代表“实际上该句子所表达的情感”，对应变量labels。\\n********************************\")\n",
    "#predicts是预测的（0或1），predict_probs是概率值\n",
    "predicts,predict_probs = predict(Trained_model,test_loader)\n",
    "#predicts\n",
    "#predict_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b91ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
