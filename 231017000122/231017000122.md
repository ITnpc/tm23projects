## 利用RNN生成诗歌

| 姓名 | 专业           | 学号         |
|------|----------------|--------------|
| 晋晨 | 计算机应用技术 | 231017000122 |

### 研究背景
随着人工智能技术和自然语言处理的发展，利用循环神经网络（RNN）生成古诗词序列、歌词、现代诗、散文等文字作品变得越来越普遍。RNN是一种可以建模序列数据的神经网络模型，具有记忆状态和动态输入输出的能力，非常适合处理文本序列生成的任务。通过训练RNN模型，我们可以生成各种模式的文字作品，从而增加了文学创作的多样性和创新性。

### 研究目标及内容
本研究的目标是利用RNN生成各类文字作品，包括古诗词序列、歌词、现代诗、散文等。

### 项目结构
```
config.py 配置文件
dataHandler.py 数据预处理
model.py 模型
train.py 训练模型
samples.py 采样诗歌
# 文件夹
data 存放预处理好的数据文件：char_dict, car_to_ix, ix_to_char
model 存放保存的模型
```

### 准备数据集
本文使用了古诗词数据来自于https://github.com/chinese-poetry/chinese-poetry  这里使用了其中整理好的全唐诗数据

| 文件信息      | 文件大小 |
|---------------|----------|
| 全唐诗 | 52M      |


### 数据集使用

在代码中设置本地语料库路径，在程序调用中会使用本地指定路径下面的语料库数据。

```
# config.py
data_path = "/root/tm23projects/231017000122/chinese-poetry/json/"
```

### 数据处理

```
    def sentence_parse(para):
        """对文本进行处理，取出脏数据"""
        # 去掉括号中的部
        result, number = re.subn("（.*）", "", para)
        result, number = re.subn("{.*}", "", result)
        result, number = re.subn("《.*》", "", result)
        result, number = re.subn("《.*》", "", result)
        result, number = re.subn("[\]\[]", "", result)
        # 去掉数字
        r = ""
        for s in result:
            if s not in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-']:
                r += s;
        # 处理两个句号为1个句号
        r, number = re.subn("。。", "。", r)

        # 返回预处理好的文本
        return r

def get_data(config):
    # 1.获取数据
    data = parse_raw_data(config.data_path, config.category, config.author, config.constrain)

    # 2.构建词典
    chars = {c for line in data for c in line}
    char_to_ix = {char: ix for ix, char in enumerate(chars)}
    char_to_ix['<EOP>'] = len(char_to_ix)
    char_to_ix['<START>'] = len(char_to_ix)
    char_to_ix['</s>'] = len(char_to_ix)

    ix_to_chars = {ix: char for char, ix in list(char_to_ix.items())}

    # 3.处理样本
    # 3.1 每首诗加上首位符号
    for i in range(0, len(data)):
        data[i] = ['<START>'] + list(data[i]) + ['<EOP>']

    # 3.2 文字转id
    data_id = [[char_to_ix[w] for w in line] for line in data]

    # 3.3 补全既定长度
    pad_data = pad_sequences(data_id,
                             maxlen=config.poetry_max_len,
                             padding='pre',
                             truncating='post',
                             value=len(char_to_ix) - 1)

    # 3.4 保存于返回
    np.savez_compressed(config.processed_data_path,
                        data=pad_data,
                        word2ix=char_to_ix,
                        ix2word=ix_to_chars)

    return pad_data, char_to_ix, ix_to_chars
```




#### 数据集：

```
class PoetryModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, device, layer_num):
        super(PoetryModel, self).__init__()
        self.hidden_dim = hidden_dim

        # 创建embedding层
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        # 创建lstm层,参数是输入输出的维度
        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=layer_num)
        # 创建一个线性层
        self.linear1 = nn.Linear(self.hidden_dim, vocab_size)
        # 创建一个dropout层，训练时作用在线性层防止过拟合
        self.dropout = nn.Dropout(0.2)

        self.device = device

    def forward(self, inputs, hidden):
        seq_len, batch_size = inputs.size()
        # 将one-hot形式的input在嵌入矩阵中转换成嵌入向量，torch.Size([length, batch_size, embedding_size])
        embeds = self.embeddings(inputs)

        # 经过lstm层，该层有2个输入，当前x和t=0时的(c,a),
        # output:torch.Size([length, batch_size, hidden_idm]), 每一个step的输出
        # hidden: tuple(torch.Size([layer_num, 32, 256]) torch.Size([1, 32, 256])) # 最后一层输出的ct 和 ht, 在这里是没有用的
        output, hidden = self.lstm(embeds, hidden)

        # 经过线性层，relu激活层 先转换成（max_len*batch_size, 256)维度，再过线性层（length, vocab_size)
        output = F.relu(self.linear1(output.view(seq_len*batch_size, -1)))

        # 输出最终结果，与hidden结果
        return output, hidden

    def init_hidden(self, layer_num, batch_size):
        return (Variable(torch.zeros(layer_num, batch_size, self.hidden_dim)).cuda(),
                Variable(torch.zeros(layer_num, batch_size, self.hidden_dim)).cuda())
```


#### 训练50次过程

```
样本数：57367
词典大小： 9222
epoch: 0,loss: 9.141785
春日不不知，不是不可知。江里無不見，不是不可知。
epoch: 1,loss: 2.719019
epoch: 2,loss: 2.598895
epoch: 3,loss: 2.610991
epoch: 4,loss: 2.751571
epoch: 5,loss: 2.291423
春日日如何，何人不可知。江中不可見，不得不知君。
epoch: 6,loss: 2.753784
epoch: 7,loss: 1.916357
epoch: 8,loss: 2.329572
epoch: 9,loss: 2.471901
epoch: 10,loss: 2.580593
春風月月，雲上雲中。江山不見，不見一年。
epoch: 11,loss: 2.545847
epoch: 12,loss: 2.510832
epoch: 13,loss: 2.468285
epoch: 14,loss: 2.313825
epoch: 15,loss: 2.529614
春風月，水中風。江山水，水中天。花中天，水中天。
epoch: 16,loss: 2.283670
epoch: 17,loss: 2.377592
epoch: 18,loss: 2.326947
epoch: 19,loss: 2.481864
epoch: 20,loss: 2.454527
春風風雨月，月月不可知。江水不可見，山山無處人。
epoch: 21,loss: 2.559447
epoch: 22,loss: 2.392588
epoch: 23,loss: 2.869742
epoch: 24,loss: 2.251814
epoch: 25,loss: 2.343519
春日不可見，不知不可知。江山不可見，一日不可知。
epoch: 26,loss: 2.560892
epoch: 27,loss: 2.472958
epoch: 28,loss: 2.366127
epoch: 29,loss: 2.372151
epoch: 30,loss: 2.325617
春風不可見，不見君人間。江山不可見，不見君人間。
epoch: 31,loss: 2.608565
epoch: 32,loss: 2.454613
epoch: 33,loss: 2.526759
epoch: 34,loss: 2.458177
epoch: 35,loss: 2.399034
春風落，日月落花花。江上山中不可見，一年一里不相逢。
epoch: 36,loss: 2.406497
epoch: 37,loss: 2.541617
epoch: 38,loss: 2.503782
epoch: 39,loss: 2.553324
epoch: 40,loss: 2.521841
春風落日落，風雨不相思。江上不相見，不知人不知。
epoch: 41,loss: 2.498725
epoch: 42,loss: 2.433702
epoch: 43,loss: 2.233962
epoch: 44,loss: 2.387502
epoch: 45,loss: 2.478868
春風落日日，水水清風雨。江上水中風，水中風月落。
epoch: 46,loss: 2.338008
epoch: 47,loss: 2.333730
epoch: 48,loss: 2.493744
epoch: 49,loss: 2.376427
```


#### 使用模型生成

```
root@iZ6wefoz8s2ck068uf9c7pZ:~/poetry# python3 sample.py 
君子不能得，無人不可知。不知一里路，不得無人情。
```

#### 结果分析

本研究的目标是利用RNN生成各类文字作品，包括古诗词序列、歌词、现代诗、散文。该模型中，通过调整训练模型的参数，通过多次训练后，改善模型分析的准确性，以便提供更加丰富的古诗词序列、歌词、现代诗、散文。

### 总结

通过本次项目实验，对上课讲解的知识有了更深一步的了解，希望在后续的工作中，可以结合目前了解和学习的知识，运用到工作实践当中。在此感谢老师的辛苦付出和耐心指导。
