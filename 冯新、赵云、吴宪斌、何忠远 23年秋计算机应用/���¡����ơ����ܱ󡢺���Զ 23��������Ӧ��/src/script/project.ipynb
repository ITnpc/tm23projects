{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "030b3c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "max_train_epochs = 5\n",
    "warmup_proportion = 0.05\n",
    "gradient_accumulation_steps = 1\n",
    "train_batch_size = 32\n",
    "valid_batch_size = train_batch_size\n",
    "test_batch_size = train_batch_size\n",
    "data_workers= 2\n",
    "\n",
    "\n",
    "\n",
    "learning_rate=2e-5\n",
    "weight_decay=0.01\n",
    "max_grad_norm=1.0\n",
    "\n",
    "    \n",
    "cur_time = time.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "model_path = '/home/zhy/anaconda3/envs/trans/trans-p/Neural_Chinese_Address_Parsing_BERT_state_dict.pkl'\n",
    "\n",
    "from transformers import BertConfig, BertTokenizer, BertModel, BertForTokenClassification\n",
    "cls_token='[CLS]'\n",
    "eos_token='[SEP]'\n",
    "unk_token='[UNK]'\n",
    "pad_token='[PAD]'\n",
    "mask_token='[MASK]'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "config = BertConfig.from_pretrained('bert-base-chinese')\n",
    "TheModel = BertModel\n",
    "ModelForTokenClassification = BertForTokenClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31589935",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['B-assist', 'I-assist', 'B-cellno', 'I-cellno', 'B-city', 'I-city', 'B-community', 'I-community', 'B-country', 'I-country', 'B-devZone', 'I-devZone', 'B-district', 'I-district', 'B-floorno', 'I-floorno', 'B-houseno', 'I-houseno', 'B-otherinfo', 'I-otherinfo', 'B-person', 'I-person', 'B-poi', 'I-poi', 'B-prov', 'I-prov', 'B-redundant', 'I-redundant', 'B-road', 'I-road', 'B-roadno', 'I-roadno', 'B-roomno', 'I-roomno', 'B-subRoad', 'I-subRoad', 'B-subRoadno', 'I-subRoadno', 'B-subpoi', 'I-subpoi', 'B-subroad', 'I-subroad', 'B-subroadno', 'I-subroadno', 'B-town', 'I-town']\n",
    "label2id = {}\n",
    "for i, l in enumerate(labels):\n",
    "    label2id[l] = i\n",
    "num_labels = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4349015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSeqTagging(ModelForTokenClassification):\n",
    "    def __init__(self):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = TheModel.from_pretrained('bert-base-chinese')\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "        self.init_weights()\n",
    "            \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        batch_size, max_len, feature_dim = sequence_output.shape\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        active_loss = attention_mask.view(-1) == 1\n",
    "        active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            active_labels = labels.view(-1)[active_loss]\n",
    "            loss = loss_fct(active_logits, active_labels)\n",
    "            return loss\n",
    "        else:\n",
    "            return active_logits\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a395685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval():\n",
    "    result = []\n",
    "    for step, batch in enumerate(tqdm(test_data_loader)):\n",
    "        input_ids, attention_mask, label = (b.to(device) for b in batch[:-1])\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            logits = F.softmax(logits, dim=-1)\n",
    "        logits = logits.data.cpu()\n",
    "        logit_list = []\n",
    "        sum_len = 0\n",
    "        for m in attention_mask:\n",
    "            l = m.sum().cpu().item()\n",
    "            logit_list.append(logits[sum_len:sum_len+l])\n",
    "            sum_len += l\n",
    "        assert sum_len == len(logits)\n",
    "        for i, l in enumerate(logit_list):\n",
    "            rr = torch.argmax(l, dim=1)\n",
    "            for j, w in enumerate(test_list[batch[-1][i]][0]):\n",
    "                result.append([w, labels[label[i][j+1].cpu().item()],labels[rr[j+1]]])\n",
    "            result.append([])\n",
    "    print(result[:20])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a9a897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.examples[index]\n",
    "        sentence = example[0]\n",
    "        #vaild_id = example[1]\n",
    "        label = example[1]\n",
    "        \n",
    "        pad_len = max_token_len - len(sentence)\n",
    "        total_len = len(sentence)+2\n",
    "        \n",
    "        input_token = [cls_token] + sentence + [eos_token] + [pad_token] * pad_len\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_token)\n",
    "        attention_mask = [1] + [1] * len(sentence) + [1] + [0] * pad_len\n",
    "        # vaild_mask = [0] + vaild_id + [0] + [0] * pad_len\n",
    "        # active_mask = [1] * len(label) + [0] * (max_token_len+2-len(label))\n",
    "        label = [-100] + label + [-100] + [-100] * pad_len\n",
    "        assert max_token_len + 2 == len(input_ids) == len(attention_mask) == len(input_token)# == len(vaild_mask)\n",
    "        \n",
    "        return input_ids, attention_mask, total_len, label, index\n",
    "    \n",
    "def the_collate_fn(batch):\n",
    "    total_lens = [b[2] for b in batch]\n",
    "    total_len = max(total_lens)\n",
    "    input_ids = torch.LongTensor([b[0] for b in batch])\n",
    "    attention_mask = torch.LongTensor([b[1] for b in batch])\n",
    "    label = torch.LongTensor([b[3] for b in batch])\n",
    "    input_ids = input_ids[:,:total_len]\n",
    "    attention_mask = attention_mask[:,:total_len]\n",
    "    label = label[:,:total_len]\n",
    "\n",
    "    indexs = [b[4] for b in batch]\n",
    "\n",
    "    return input_ids, attention_mask, label, indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d574ac80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSeqTagging()\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebd266ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['中', '华', '人', '民', '共', '和', '国', '河', '北', '省', '廊', '坊', '市', '市', '河', '西', '区', '展', '览', '路', '街', '道', '美', '丽', '小', '区'], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]\n"
     ]
    }
   ],
   "source": [
    "#cent=\"中华人民共和国河北省廊坊市市河西区展览路街道美丽小区1\"\n",
    "cent = input(\"Enter any value: \")\n",
    "inputs = tokenizer(cent, add_special_tokens=False)\n",
    "i2 =[]\n",
    "for aa in cent:\n",
    "    i2.append(aa)\n",
    "\n",
    "a1=inputs['attention_mask']\n",
    "\n",
    "a2 =[]\n",
    "for aa in a1:\n",
    "    a2.append(aa)\n",
    "test_list=[]\n",
    "test_list.append([i2,a2])\n",
    "test_dataset = MyDataSet(test_list)\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle = False,\n",
    "    num_workers=data_workers,\n",
    "    collate_fn=the_collate_fn,\n",
    ")\n",
    "print(test_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfba35ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_token_len 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['中', 'I-assist', 'B-country'], ['华', 'I-assist', 'I-country'], ['人', 'I-assist', 'I-prov'], ['民', 'I-assist', 'I-prov'], ['共', 'I-assist', 'I-prov'], ['和', 'I-assist', 'I-prov'], ['国', 'I-assist', 'I-prov'], ['河', 'I-assist', 'B-prov'], ['北', 'I-assist', 'I-prov'], ['省', 'I-assist', 'I-prov'], ['廊', 'I-assist', 'B-city'], ['坊', 'I-assist', 'I-city'], ['市', 'I-assist', 'I-city'], ['市', 'I-assist', 'I-city'], ['河', 'I-assist', 'B-district'], ['西', 'I-assist', 'I-district'], ['区', 'I-assist', 'I-district'], ['展', 'I-assist', 'B-town'], ['览', 'I-assist', 'I-town'], ['路', 'I-assist', 'I-town']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_token_len=0\n",
    "for ls in test_list:\n",
    "    max_token_len = max(max_token_len, len(ls[0]))\n",
    "print('max_token_len', max_token_len)\n",
    "result = eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "610a9204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "country :\n",
      "中华人民共和国\n",
      "prov :\n",
      "河北省\n",
      "city :\n",
      "廊坊市市\n",
      "district :\n",
      "河西区\n",
      "town :\n",
      "展览路街道\n",
      "poi :\n",
      "美丽小区"
     ]
    }
   ],
   "source": [
    "ii = -1\n",
    "aas = []\n",
    "tt = []\n",
    "for aa in result[:-1]:\n",
    "        a1,a2,a3=aa\n",
    "        if a3[0] == \"B\":\n",
    "                print('')\n",
    "                print(a3[2:],':')\n",
    "        print(a1,end='')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
